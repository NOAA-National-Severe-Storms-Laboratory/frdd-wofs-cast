{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1ef8a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'true'\n",
    "# Set this lower, to allow for PyTorch Model to fit into memory\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.90' \n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import sys\n",
    "package_path = os.path.dirname(os.path.dirname(os.getcwd())) \n",
    "sys.path.insert(0, package_path)\n",
    "from glob import glob \n",
    "\n",
    "from wofscast.model import WoFSCastModel\n",
    "from wofscast.data_generator import load_chunk, dataset_to_input, add_local_solar_time\n",
    "from wofscast.common.wofs_data_loader import WoFSDataLoader\n",
    "from wofscast.common.wofs_analysis_loader import WoFSAnalysisLoader\n",
    "from wofscast.common.mrms_data_loader import MRMSDataLoader \n",
    "\n",
    "# For the diffusion model. \n",
    "from wofscast.diffusion import DiffusionModel \n",
    "\n",
    "# Utils for loading data, plotting, animations. \n",
    "from wofscast.common.helpers import (get_case_date, \n",
    "                                     to_datetimes, \n",
    "                                     get_qpe_datetimes, \n",
    "                                     border_difference_check,\n",
    "                                     compute_nmep, \n",
    "                                     convert_rain_amount_to_inches, \n",
    "                                     convert_T2_K_to_F,\n",
    "                                     _border_mask, \n",
    "                                     parse_arguments, \n",
    "                                     load_configuration\n",
    "                                    )\n",
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "from scipy.ndimage import uniform_filter\n",
    "\n",
    "# For plotting. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import itertools \n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/MontePython')\n",
    "import monte_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f618e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This configuration class contains all the user-settings required to run this notebook. \n",
    "\n",
    "@dataclass\n",
    "class EvaluatorConfig :    \n",
    "    # Path to the WoFSCast model weights.\n",
    "    model_path = '/work/cpotvin/WOFSCAST/model/wofscast_test_v178.npz'    \n",
    "    timestep = 10 \n",
    "    steps_per_hour = 60 // timestep # 60 min / 5 min time steps\n",
    "    hours = 1\n",
    "    n_steps = int(steps_per_hour * hours)\n",
    "    year = '2021'\n",
    "    mem = 9 \n",
    "    resize = True\n",
    "    full_domain = False \n",
    "    n_times = 12 \n",
    "    wofs_dbz_thresh = 47 # Same for both WoFS and WoFSCast \n",
    "    mrms_dbz_thresh = 40 \n",
    "    matching_dist = 7\n",
    "    min_area = 12 \n",
    "    domain_size = 150 \n",
    "    FSS_vars = ['COMPOSITE_REFL_10CM', 'RAIN_AMOUNT']\n",
    "    FSS_thres = {'COMPOSITE_REFL_10CM': 40, 'RAIN_AMOUNT': 25.4/2}\n",
    "    window_list = [7, 15, 27]\n",
    "    \n",
    "\n",
    "# List of pandas datetime objects for plotting the time step. \n",
    "evaluator_config = EvaluatorConfig() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5cce028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_preserving_time(x: xr.DataArray) -> xr.DataArray:\n",
    "    return x.mean([d for d in x.dims if d != 'time'], skipna=True)\n",
    "\n",
    "class ThunderScoreEvaluator:\n",
    "    \"\"\"A generalized class for evaluating thunderstorm-scale forecasts over multiple timesteps.\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator_config, \n",
    "                 methods=['object_matching'], \n",
    "                 preprocess_fn=add_local_solar_time):\n",
    "        \"\"\"\n",
    "        Initializes the evaluator with configuration and methods.\n",
    "\n",
    "        Parameters:\n",
    "        evaluator_config: Configuration for the evaluation process.\n",
    "        methods: List of methods for comparing datasets.\n",
    "        preprocess_fn: Preprocessing function to be applied on data before evaluation.\n",
    "        \"\"\"\n",
    "        self.evaluator_config = evaluator_config\n",
    "        self.methods = methods \n",
    "        self.preprocess_fn = preprocess_fn \n",
    "        self.cached_predictions = {}  # Cache for storing predictions\n",
    "        \n",
    "        matcher = monte_python.ObjectMatcher(cent_dist_max = self.evaluator_config.matching_dist, \n",
    "                                     min_dist_max = self.evaluator_config.matching_dist, \n",
    "                                     time_max=0, \n",
    "                                     score_thresh=0.2, \n",
    "                                     one_to_one = True)\n",
    "        \n",
    "        self.obj_verifier = monte_python.ObjectVerifier(matcher)  # Verifier for object-based metrics\n",
    "    \n",
    "        self.qcer = monte_python.QualityControler()\n",
    "        self.qc_params = [('min_area', self.evaluator_config.min_area)]\n",
    "        \n",
    "        # Create a border mask for the domain (slow to constantly recreate this!!!)\n",
    "        self.BORDER_MASK = _border_mask((self.evaluator_config.domain_size, \n",
    "                                         self.evaluator_config.domain_size), N=5)  # Adjust N as needed\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the prediction model.\"\"\"\n",
    "        model = WoFSCastModel()\n",
    "        if self.evaluator_config.full_domain:\n",
    "            model.load_model(self.evaluator_config.model_path, **{'tiling': (2, 2)})\n",
    "        else:    \n",
    "            model.load_model(self.evaluator_config.model_path)\n",
    "        \n",
    "        # Set the task config.\n",
    "        self.task_config = model.task_config\n",
    "        return model     \n",
    "    \n",
    "    def _load_inputs_targets_forcings(self, data_path):\n",
    "        \"\"\"Load inputs, targets, and forcings.\"\"\" \n",
    "        inputs, targets, forcings = self.data_loader.load_inputs_targets_forcings(data_path)\n",
    "        return inputs, targets, forcings \n",
    "    \n",
    "    def _load_mrms_data(self, datetime_rng):\n",
    "        \"\"\"Load MRMS data for the given datetime range.\"\"\"\n",
    "        try:\n",
    "            loader = MRMSDataLoader(\n",
    "                self.data_loader.case_date, \n",
    "                datetime_rng, \n",
    "                domain_size=self.evaluator_config.domain_size, \n",
    "                resize_domain=self.evaluator_config.resize\n",
    "            )\n",
    "            mrms_dz = loader.load()  # Shape: (NT, NY, NX)\n",
    "        except OSError:\n",
    "            print(f'Unable to load MRMS data for {datetime_rng}')\n",
    "            return None\n",
    "        \n",
    "        return mrms_dz \n",
    "    \n",
    "    def _load_wofs_analysis_data(self):\n",
    "        # Placeholder for loading WoFS analysis data\n",
    "        # NOT FINISHED!\n",
    "        pass\n",
    "    \n",
    "    def _init_results_dict(self):\n",
    "        # Create an empty results_dict\n",
    "        results_dict = {\n",
    "            'Full Domain': {v: np.zeros((self.evaluator_config.n_times,)) \n",
    "                            for v in self.target_vars},\n",
    "            'Convective Regions': {v: np.zeros((self.evaluator_config.n_times,)) \n",
    "                                   for v in self.target_vars},\n",
    "        }\n",
    "        \n",
    "        # Initialize the contingency table metric storage. \n",
    "        self.obj_match_metrics = ['hits', 'misses', 'false_alarms']\n",
    "        obj_match_keys = [f'{pair[0]}_vs_{pair[1]}_object_matching' for pair in comparison_pairs] \n",
    "        for m, s in itertools.product(self.obj_match_metrics, obj_match_keys):\n",
    "            results_dict[f'{s}_{m}'] = np.zeros((self.evaluator_config.n_times))\n",
    "        \n",
    "        for v in self.evaluator_config.FSS_vars: \n",
    "            results_dict[f'{v}_FSS_numer'] = {w: np.zeros((self.evaluator_config.n_times)) \n",
    "                                          for w in self.evaluator_config.window_list}\n",
    "            results_dict[f'{v}_FSS_denom'] = {w: np.zeros((self.evaluator_config.n_times)) \n",
    "                                          for w in self.evaluator_config.window_list}\n",
    "        \n",
    "        return results_dict\n",
    "    \n",
    "    def score(self, data_paths, \n",
    "              comparison_pairs=[('predictions', 'targets'), \n",
    "                                ('predictions', 'mrms'),\n",
    "                                ('targets', 'mrms')]\n",
    "             ):\n",
    "        \"\"\"\n",
    "        Evaluate the model and compute scores for multiple datasets over time.\n",
    "\n",
    "        Parameters:\n",
    "        data_paths: List of data paths for the predictions and targets.\n",
    "        comparison_pairs: A list of tuples where each tuple specifies a pair of datasets to compare.\n",
    "        Example: [('predictions', 'targets'), ('predictions', 'mrms'), ('targets', 'mrms')]\n",
    "        \"\"\"\n",
    "        # Load model first to get the task config for \n",
    "        # loading the input, target, and forcing datasets below.\n",
    "        model = self._load_model()\n",
    "        \n",
    "        self.data_loader = WoFSDataLoader(\n",
    "            self.evaluator_config, \n",
    "            self.task_config, \n",
    "            self.preprocess_fn, \n",
    "            load_ensemble=False\n",
    "        )  \n",
    "        \n",
    "        self.target_vars = self.task_config.target_variables\n",
    "        \n",
    "        # Create an empty results_dict\n",
    "        results_dict = self._init_results_dict()\n",
    "        \n",
    "        # Used to normalize the accumulated RMSE. \n",
    "        N = len(data_paths)\n",
    "        \n",
    "        for i, path in enumerate(tqdm(data_paths, desc='Evaluating Model')):\n",
    "            print(f\"Evaluating {path}...\")\n",
    "            \n",
    "            datetime_rng = to_datetimes(path, n_times=self.evaluator_config.n_times+2)\n",
    "            \n",
    "            # Load inputs, targets, and forcings\n",
    "            inputs, targets, forcings = self._load_inputs_targets_forcings(path)\n",
    "            predictions = model.predict(inputs, targets, forcings, replace_bdry=True)\n",
    "            predictions = predictions.transpose('batch', 'time', 'level', 'lat', 'lon')\n",
    "            targets = targets.transpose('batch', 'time', 'level', 'lat', 'lon')\n",
    "            \n",
    "            predictions = predictions.isel(batch=0)\n",
    "            targets = targets.isel(batch=0)\n",
    "            \n",
    "            # Load MRMS data for the forecast time series\n",
    "            mrms_dz = self._load_mrms_data(datetime_rng)\n",
    "            \n",
    "            # Compute the RMSE statistics (can be computed all at once). \n",
    "            results_dict = self.accumulate_rmse(targets, predictions, results_dict)\n",
    "            \n",
    "            # Evaluate each time step for other metrics. \n",
    "            for t in range(self.evaluator_config.n_times):\n",
    "                # Extract data for the current timestep\n",
    "                timestep_datasets = {\n",
    "                    'predictions': predictions.isel(time=t),\n",
    "                    'targets': targets.isel(time=t),\n",
    "                    'mrms': mrms_dz[t, :, :] if mrms_dz is not None else None\n",
    "                }\n",
    "                \n",
    "                # Perform comparisons for each dataset pair \n",
    "                # Only used for the object matching statistics \n",
    "                # at the moment. \n",
    "                for pair in comparison_pairs:\n",
    "                    dataset_1 = timestep_datasets[pair[0]]\n",
    "                    dataset_2 = timestep_datasets[pair[1]]\n",
    "                    results_dict = self._compare_datasets(t, pair[0], pair[1], dataset_1, dataset_2, \n",
    "                                                          results_dict)\n",
    "                \n",
    "                # Calculate FSS for each variable and window\n",
    "                for var in self.evaluator_config.FSS_vars:\n",
    "                    for window in self.evaluator_config.window_list: \n",
    "                        numer, denom = self.fractions_skill_score(timestep_datasets['predictions'][var], \n",
    "                                                                  timestep_datasets['targets'][var], \n",
    "                                                                  window, \n",
    "                                                                  self.evaluator_config.FSS_thres[var])\n",
    "                        results_dict[f'{var}_FSS_numer'][window][t] += numer\n",
    "                        results_dict[f'{var}_FSS_denom'][window][t] += denom\n",
    "                   \n",
    "        for key in ['Full Domain', 'Convective Regions']:\n",
    "            for v in self.target_vars:\n",
    "                results_dict[key][v]/=N\n",
    "    \n",
    "        return EvaluationResults(results_dict, self.evaluator_config)\n",
    "    \n",
    "    def _compare_datasets(self, t, name_1, name_2, dataset_1, dataset_2, results_dict):\n",
    "        \"\"\"\n",
    "        Generalized comparison between two datasets using the specified methods.\n",
    "\n",
    "        Parameters:\n",
    "        t: The current timestep being evaluated.\n",
    "        name_1: Name of the first dataset (e.g., 'predictions').\n",
    "        name_2: Name of the second dataset (e.g., 'targets').\n",
    "        dataset_1: The actual data for the first dataset.\n",
    "        dataset_2: The actual data for the second dataset.\n",
    "        results_dict: Dictionary to store results for each comparison.\n",
    "\n",
    "        Returns:\n",
    "        Updated results_dict with the comparison results.\n",
    "        \"\"\"\n",
    "        # Perform comparisons\n",
    "        for method in ['object_matching']:\n",
    "            comparison_fn = getattr(self, method, None)\n",
    "  \n",
    "            result = comparison_fn(dataset_1, dataset_2, name_1, name_2, results_dict)\n",
    "                \n",
    "            # Accumulate the hits, false alarms, and misses\n",
    "            for key in self.obj_match_metrics:\n",
    "                results_dict[f'{name_1}_vs_{name_2}_{method}_{key}'][t] += result[key]\n",
    "               \n",
    "        return results_dict\n",
    "\n",
    "    \n",
    "    def _object_id(self, dataset, dataset_type=None): \n",
    "        \"\"\"\n",
    "        Identifies objects in the dataset using a threshold based on the dataset type.\n",
    "\n",
    "        Parameters:\n",
    "        dataset: The input dataset, which could be an xarray DataArray or a numpy array.\n",
    "        dataset_type: A string indicating the type of dataset ('mrms', 'predictions', 'targets').\n",
    "                  If None, the function will infer the type based on the dataset.\n",
    "\n",
    "        Returns:\n",
    "        labels: Labeled objects in the dataset.\n",
    "        props: Properties of the identified objects.\n",
    "        \"\"\"\n",
    "        # Determine the threshold based on dataset type or dataset itself\n",
    "        if dataset_type == 'mrms' or isinstance(dataset, np.ndarray):\n",
    "            thresh = self.evaluator_config.mrms_dbz_thresh\n",
    "        else:\n",
    "            thresh = self.evaluator_config.wofs_dbz_thresh\n",
    "    \n",
    "        # Handle numpy arrays (mrms_dz) differently if necessary\n",
    "        if isinstance(dataset, np.ndarray):\n",
    "            data = dataset\n",
    "        else:\n",
    "            data = dataset['COMPOSITE_REFL_10CM']\n",
    "    \n",
    "        # Apply the object identification process\n",
    "        labels, props = monte_python.label(\n",
    "            input_data=data,\n",
    "            method='single_threshold',\n",
    "            return_object_properties=True, \n",
    "            params={'bdry_thresh': thresh}\n",
    "        )\n",
    "        \n",
    "        # Apply QC'ing \n",
    "        labels, props = self.qcer.quality_control(\n",
    "            data, labels, props, self.qc_params)\n",
    "        \n",
    "        return labels, props\n",
    "\n",
    "    \n",
    "    def object_matching(self, dataset_1, dataset_2, name_1, name_2, results_dict):\n",
    "        \"\"\"Perform object matching between two datasets.\"\"\"\n",
    "        labels_1, props_1 = self._object_id(dataset_1, name_1)\n",
    "        labels_2, props_2 = self._object_id(dataset_2, name_2)\n",
    "\n",
    "        self.obj_verifier.update_metrics(labels_2, labels_1)\n",
    "        result = {key: getattr(self.obj_verifier, f\"{key}_\") for key in [\"hits\", \"false_alarms\", \"misses\"]}\n",
    "        self.obj_verifier.reset_metrics()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def accumulate_rmse(self, dataset_1, dataset_2, results_dict):\n",
    "        \"\"\"Accumulate RMSE for each prediction.\"\"\"\n",
    "        for var in self.target_vars:\n",
    "        \n",
    "            # Compute full domain RMSE while ignoring borders and preserving the time dimension\n",
    "            rmse = self.rmse_ignoring_borders(dataset_1[var], dataset_2[var])\n",
    "\n",
    "            # Compute RMSE where comp. refl > 3\n",
    "            pred_refl_mask = (dataset_1['COMPOSITE_REFL_10CM'] > 3)\n",
    "            tar_refl_mask = (dataset_2['COMPOSITE_REFL_10CM'] > 3)\n",
    "        \n",
    "            # Combine the masks with logical OR to create the composite reflectivity mask\n",
    "            refl_mask = pred_refl_mask | tar_refl_mask\n",
    "        \n",
    "            # Apply the mask and compute RMSE in convection while preserving the time dimension\n",
    "            rmse_conv = self.rmse_in_convection(dataset_1[var], dataset_2[var], refl_mask)\n",
    "        \n",
    "            # Accumulate RMSE values\n",
    "            results_dict['Full Domain'][var] += rmse\n",
    "            results_dict['Convective Regions'][var] += rmse_conv\n",
    "        \n",
    "        return results_dict\n",
    "\n",
    "    def rmse_ignoring_borders(self, predictions, targets):\n",
    "                \n",
    "        # Ensure BORDER_MASK is broadcasted to the correct shape\n",
    "        border_mask = self.BORDER_MASK\n",
    "    \n",
    "        # Broadcast the mask to match the shape of predictions/targets if necessary\n",
    "        if border_mask.shape != predictions.shape:\n",
    "            border_mask = np.broadcast_to(border_mask, predictions.shape)\n",
    "    \n",
    "        # Set the errors at the borders to NaN\n",
    "        err = (predictions - targets)**2\n",
    "        err = xr.where(border_mask, np.nan, err)  # Apply the border mask\n",
    "    \n",
    "        # Compute mean squared error while preserving the 'time' dimension\n",
    "        mse = mean_preserving_time(err)\n",
    "    \n",
    "        # Calculate the RMSE\n",
    "        rmse = np.sqrt(mse)\n",
    "    \n",
    "        return rmse\n",
    "\n",
    "    def rmse_in_convection(self, predictions, targets, refl_mask):\n",
    "    \n",
    "        # Set the errors at the borders to NaN\n",
    "        err = (predictions - targets)**2\n",
    "        err = xr.where(refl_mask, err, np.nan)  # Apply the refl mask\n",
    "    \n",
    "        # Compute mean squared error while preserving 'time' dimension\n",
    "        mse = mean_preserving_time(err)\n",
    "    \n",
    "        # Calculate the RMSE\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "   \n",
    "    def fractions_skill_score(self, dataset_1, dataset_2, window, thresh):\n",
    "        \"\"\"Compute the FSS.\"\"\"\n",
    "        binary_pred = (dataset_1 >= thresh).astype(float)\n",
    "        binary_true = (dataset_2 >= thresh).astype(float)\n",
    "    \n",
    "        NP_pred = uniform_filter(binary_pred, window, mode='constant')\n",
    "        NP_true = uniform_filter(binary_true, window, mode='constant')\n",
    "\n",
    "        numer = ((NP_pred - NP_true)**2).sum()\n",
    "        denom = (NP_pred**2 + NP_true**2).sum()\n",
    "    \n",
    "        return numer, denom\n",
    "\n",
    "\n",
    "class EvaluationResults:\n",
    "    def __init__(self, results_dict, evaluator_config): \n",
    "        self.results_dict = results_dict\n",
    "        self.evaluator_config = evaluator_config \n",
    "        \n",
    "    def to_fss_dataframe(self):\n",
    "\n",
    "        FSS_data = {}\n",
    "        for var in self.evaluator_config.FSS_vars:\n",
    "            for window in self.evaluator_config.window_list: \n",
    "                se = self.results_dict[f'{var}_FSS_numer'][window]\n",
    "                potential_se = self.results_dict[f'{var}_FSS_denom'][window]\n",
    "                FSS_data[f'{var}_{window*3}km'] = 1 - se / potential_se\n",
    "\n",
    "        df = pd.DataFrame(FSS_data)        \n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def to_rmse_dataframe(self):\n",
    "        # Save the RMSE results. \n",
    "        df = self.rmse_dict_to_dataframe(self.results_dict)\n",
    "        #out_path = '/work/mflora/wofs-cast-data/verification_results'\n",
    "        #df.to_parquet(os.path.join(out_path, f\"MSE_{os.path.basename(MODEL_PATH).replace('.npz','')}{tag}.parquet\"))\n",
    "        return df \n",
    "    \n",
    "    def to_parquet(self, df, path = '/work/mflora/wofs-cast-data/verification_results/results.parquet'):\n",
    "        df.to_parquet(path)\n",
    "        return f'Saved dataframe to {path}'\n",
    "    \n",
    "    def to_json(self, df, path):\n",
    "        df.to_json(path)\n",
    "        return f'Saved dataframe to {path}'\n",
    "    \n",
    "    def replace_zeros(self, data): \n",
    "        return np.where(data==0, 1e-5, data)\n",
    "\n",
    "    def rmse_dict_to_dataframe(self, rmse_dict):\n",
    "        \"\"\"\n",
    "        Convert a nested dictionary of xarray.DataArray objects to a pandas DataFrame.\n",
    "    \n",
    "        Parameters:\n",
    "        - rmse_dict: dict, nested dictionary with RMSE values\n",
    "    \n",
    "        Returns:\n",
    "        - pd.DataFrame, DataFrame with hierarchical indexing\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for key1, nested_dict in rmse_dict.items():\n",
    "            if key1 not in ['Full Domain', 'Convective Regions']:\n",
    "                continue\n",
    "            \n",
    "            for key2, data_array in nested_dict.items():\n",
    "                # Ensure data_array is an xarray.DataArray\n",
    "                if isinstance(data_array, xr.DataArray):\n",
    "                    # Extract values and timesteps\n",
    "                    values = data_array.values\n",
    "                    timesteps = data_array.coords['time'].values if 'time' in data_array.coords else range(len(values))\n",
    "                \n",
    "                    for timestep, value in zip(timesteps, values):\n",
    "                        data.append((key1, key2, timestep, value))\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(data, columns=['Category', 'Variable', 'Time', 'RMSE'])\n",
    "    \n",
    "        # Set hierarchical index\n",
    "        df.set_index(['Category', 'Variable', 'Time'], inplace=True)\n",
    "    \n",
    "        return df \n",
    "    \n",
    "    def to_contigency_table_dataframe(self):\n",
    "        \"\"\"Calculate object-based metrics like POD, SR, CSI, and FB.\"\"\"\n",
    "        subkeys = [k.replace('_hits', '') for k in result.keys() if 'vs' in k and 'hits' in k]\n",
    "        \n",
    "        data = {}\n",
    "        for subkey in subkeys:\n",
    "            hits = self.replace_zeros(self.results_dict[f'{subkey}_hits'])\n",
    "            misses = self.replace_zeros(self.results_dict[f'{subkey}_misses'])\n",
    "            false_alarms = self.replace_zeros(self.results_dict[f'{subkey}_false_alarms'])\n",
    "\n",
    "            pod = hits / (hits + misses)\n",
    "            sr = hits / (hits + false_alarms)\n",
    "            csi = hits / (hits + misses + false_alarms)\n",
    "            fb = pod / sr\n",
    "\n",
    "            subkey = subkey.replace('_object_matching', '')\n",
    "            \n",
    "            data[f'{subkey}_POD'] = pod\n",
    "            data[f'{subkey}_SR'] = sr\n",
    "            data[f'{subkey}_CSI'] = csi\n",
    "            data[f'{subkey}_FB'] = fb\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "85d927ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/work/mflora/wofs-cast-data/datasets_2hr_zarr/'\n",
    "fname = 'wrfwof_2021-05-15_020000_to_2021-05-15_041000__10min__ens_mem_09.zarr'\n",
    "data_paths = [os.path.join(base_path, '2021', fname)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e08893e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating Model:   0%|                                                                                                                      | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating /work/mflora/wofs-cast-data/datasets_2hr_zarr/2021/wrfwof_2021-05-15_020000_to_2021-05-15_041000__10min__ens_mem_09.zarr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.43s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluator = ThunderScoreEvaluator(evaluator_config)\n",
    "\n",
    "# Define comparison pairs\n",
    "comparison_pairs = [('predictions', 'targets'), ('predictions', 'mrms'), ('targets', 'mrms')]\n",
    "\n",
    "# Evaluate and compare the datasets over time\n",
    "results = evaluator.score(data_paths, comparison_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "94d44c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.to_fss_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3d2f3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPOSITE_REFL_10CM_21km</th>\n",
       "      <th>COMPOSITE_REFL_10CM_45km</th>\n",
       "      <th>COMPOSITE_REFL_10CM_81km</th>\n",
       "      <th>RAIN_AMOUNT_21km</th>\n",
       "      <th>RAIN_AMOUNT_45km</th>\n",
       "      <th>RAIN_AMOUNT_81km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992995</td>\n",
       "      <td>0.995859</td>\n",
       "      <td>0.996322</td>\n",
       "      <td>0.985275</td>\n",
       "      <td>0.987627</td>\n",
       "      <td>0.989704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.987353</td>\n",
       "      <td>0.992474</td>\n",
       "      <td>0.993551</td>\n",
       "      <td>0.944425</td>\n",
       "      <td>0.957701</td>\n",
       "      <td>0.969695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.960419</td>\n",
       "      <td>0.972555</td>\n",
       "      <td>0.977899</td>\n",
       "      <td>0.937060</td>\n",
       "      <td>0.933582</td>\n",
       "      <td>0.942774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.956222</td>\n",
       "      <td>0.971475</td>\n",
       "      <td>0.974203</td>\n",
       "      <td>0.944802</td>\n",
       "      <td>0.962163</td>\n",
       "      <td>0.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.947307</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.981380</td>\n",
       "      <td>0.923468</td>\n",
       "      <td>0.947735</td>\n",
       "      <td>0.959892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.943043</td>\n",
       "      <td>0.972544</td>\n",
       "      <td>0.984634</td>\n",
       "      <td>0.946792</td>\n",
       "      <td>0.962542</td>\n",
       "      <td>0.972520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.920733</td>\n",
       "      <td>0.959587</td>\n",
       "      <td>0.974634</td>\n",
       "      <td>0.847474</td>\n",
       "      <td>0.876782</td>\n",
       "      <td>0.899810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.935894</td>\n",
       "      <td>0.971082</td>\n",
       "      <td>0.983281</td>\n",
       "      <td>0.743232</td>\n",
       "      <td>0.753838</td>\n",
       "      <td>0.769370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.913015</td>\n",
       "      <td>0.961353</td>\n",
       "      <td>0.983658</td>\n",
       "      <td>0.426388</td>\n",
       "      <td>0.565623</td>\n",
       "      <td>0.714383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.917596</td>\n",
       "      <td>0.968106</td>\n",
       "      <td>0.990269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051142</td>\n",
       "      <td>0.276246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.909319</td>\n",
       "      <td>0.960018</td>\n",
       "      <td>0.978066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.887938</td>\n",
       "      <td>0.954892</td>\n",
       "      <td>0.978337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    COMPOSITE_REFL_10CM_21km  COMPOSITE_REFL_10CM_45km  \\\n",
       "0                   0.992995                  0.995859   \n",
       "1                   0.987353                  0.992474   \n",
       "2                   0.960419                  0.972555   \n",
       "3                   0.956222                  0.971475   \n",
       "4                   0.947307                  0.972797   \n",
       "5                   0.943043                  0.972544   \n",
       "6                   0.920733                  0.959587   \n",
       "7                   0.935894                  0.971082   \n",
       "8                   0.913015                  0.961353   \n",
       "9                   0.917596                  0.968106   \n",
       "10                  0.909319                  0.960018   \n",
       "11                  0.887938                  0.954892   \n",
       "\n",
       "    COMPOSITE_REFL_10CM_81km  RAIN_AMOUNT_21km  RAIN_AMOUNT_45km  \\\n",
       "0                   0.996322          0.985275          0.987627   \n",
       "1                   0.993551          0.944425          0.957701   \n",
       "2                   0.977899          0.937060          0.933582   \n",
       "3                   0.974203          0.944802          0.962163   \n",
       "4                   0.981380          0.923468          0.947735   \n",
       "5                   0.984634          0.946792          0.962542   \n",
       "6                   0.974634          0.847474          0.876782   \n",
       "7                   0.983281          0.743232          0.753838   \n",
       "8                   0.983658          0.426388          0.565623   \n",
       "9                   0.990269          0.000000          0.051142   \n",
       "10                  0.978066          0.000000          0.000000   \n",
       "11                  0.978337          0.000000          0.000000   \n",
       "\n",
       "    RAIN_AMOUNT_81km  \n",
       "0           0.989704  \n",
       "1           0.969695  \n",
       "2           0.942774  \n",
       "3           0.980400  \n",
       "4           0.959892  \n",
       "5           0.972520  \n",
       "6           0.899810  \n",
       "7           0.769370  \n",
       "8           0.714383  \n",
       "9           0.276246  \n",
       "10          0.152672  \n",
       "11          0.000000  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe05fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

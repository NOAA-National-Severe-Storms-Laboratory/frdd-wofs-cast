{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347c3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import fsspec\n",
    "import dask \n",
    "\n",
    "import numpy as np\n",
    "import dataclasses\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "from wofscast.data_generator import add_local_solar_time, to_static_vars \n",
    "from wofscast import data_utils\n",
    "\n",
    "\n",
    "class WoFSCastDataGenerator:\n",
    "    \"\"\"\n",
    "    Generates batches from a WoFS dataset for training/testing machine learning models.\n",
    "    \n",
    "    Attributes:\n",
    "        task_config: TaskConfig object, including variables, pressure levels, etc.\n",
    "                     Defined in graphcast_lam.py \n",
    "        cpu_batch_size : int : Number of samples to preload into CPU memory \n",
    "        gpu_batch_size : int : Numbe of samples sent to GPU at one time. \n",
    "        seed : int : Random seed for the shuffling the dataset. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task_config, cpu_batch_size=512, gpu_batch_size=32, seed=123):\n",
    "        \n",
    "        self.task_config = task_config\n",
    "        self.cpu_batch_size = cpu_batch_size\n",
    "        self.gpu_batch_size = gpu_batch_size \n",
    "        \n",
    "        # Set the seed for reproducibility\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def __call__(self, dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset : xarray.Dataset : lazily loaded dataset using open_mfdataset\n",
    "    \n",
    "        Yields:\n",
    "            inputs, targets, forcings : xarray.Datasets \n",
    "    \n",
    "        Batcher for an xarray dataset using xbatcher. Useful for storing the full dataset in \n",
    "        CPU RAM and then offloading small subsets to the GPU RAM batch by batch.\n",
    "        \"\"\"\n",
    "        dims = ('batch', 'time', 'lat', 'lon', 'level')\n",
    "\n",
    "        total_samples = dataset.sizes['batch']\n",
    "        indices = np.arange(total_samples)\n",
    "             \n",
    "        outer_start = 0\n",
    "\n",
    "        while outer_start < total_samples:\n",
    "            outer_end = min(outer_start + self.cpu_batch_size, total_samples)\n",
    "            chunk_indices = indices[outer_start:outer_end]\n",
    "\n",
    "            # Preload the chunk of batches\n",
    "            chunk = dataset.isel(batch=chunk_indices).compute()\n",
    "\n",
    "            inner_start = 0\n",
    "            while inner_start < len(chunk_indices):\n",
    "                inner_end = min(inner_start + self.gpu_batch_size, len(chunk_indices))\n",
    "                batch = chunk.isel(batch=slice(inner_start, inner_end))\n",
    "\n",
    "                inputs, targets, forcings = data_utils.extract_inputs_targets_forcings(\n",
    "                    batch,\n",
    "                    target_lead_times=self.task_config.train_lead_times,\n",
    "                    **dataclasses.asdict(self.task_config)\n",
    "                )\n",
    "            \n",
    "                inputs = to_static_vars(inputs)\n",
    "            \n",
    "                inputs = inputs.transpose(*dims, missing_dims='ignore')\n",
    "                targets = targets.transpose(*dims, missing_dims='ignore')\n",
    "                forcings = forcings.transpose(*dims, missing_dims='ignore')\n",
    "            \n",
    "                yield inputs, targets, forcings\n",
    "\n",
    "                inner_start = inner_end\n",
    "\n",
    "            outer_start = outer_end\n",
    "    \n",
    "    def is_nested_list(self, lst):\n",
    "        \"\"\"\n",
    "        Check if a list is a nested list (i.e., contains other lists as elements).\n",
    "    \n",
    "        Args:\n",
    "            lst (list): The list to check.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if the list is a nested list, False otherwise.\n",
    "        \"\"\"\n",
    "        return any(isinstance(i, list) for i in lst)\n",
    "    \n",
    "    def open_mfdataset(self, paths, concat_time=False):\n",
    "        \"\"\"\n",
    "        Open multiple files as a single xarray dataset using Kerchunk JSON descriptors.\n",
    "        \n",
    "        Args:\n",
    "            paths (list of str): List of paths to the JSON files.\n",
    "            concat_time : bool : Whether to concatenate along a time dimension \n",
    "                                 before concatenating along a batch dimension. \n",
    "                                 Must pass a nested list. \n",
    "        \n",
    "        Returns:\n",
    "            xarray.Dataset: The combined dataset.\n",
    "        \"\"\"\n",
    "        @dask.delayed\n",
    "        def load_dataset_from_json(json_path):\n",
    "            \"\"\"Load a dataset from a Kerchunk JSON descriptor.\"\"\"\n",
    "            mapper = fsspec.get_mapper('reference://', fo=json_path, remote_protocol='file')\n",
    "            ds = xr.open_dataset(mapper, engine='zarr', consolidated=False, chunks={}, decode_times=False)\n",
    "            ds = add_local_solar_time(ds)\n",
    "            return ds\n",
    "\n",
    "        def load_and_concatenate(json_files, concat_dim):\n",
    "            \"\"\"Load multiple datasets from JSON files and concatenate them along a specified dimension.\"\"\"\n",
    "            datasets = [load_dataset_from_json(json_file) for json_file in json_files]\n",
    "            datasets = dask.compute(*datasets)\n",
    "            combined_dataset = xr.concat(datasets, dim=concat_dim)\n",
    "            for ds in datasets:\n",
    "                ds.close()\n",
    "            return combined_dataset\n",
    "\n",
    "        if concat_time:\n",
    "            if not self.is_nested_time(paths):\n",
    "                raise ValueError('paths must be a nested list if concatenating along a time dimension.')\n",
    "            datasets_per_time = [load_and_concatenate(p, concat_dim='Time') for p in paths]\n",
    "            dataset = xr.concat(datasets_per_time, dim='batch')\n",
    "            dataset = dataset.rename({'Time': 'time'})\n",
    "        else:\n",
    "            dataset = load_and_concatenate(paths, concat_dim='batch')\n",
    "\n",
    "        total_samples = dataset.sizes['batch']\n",
    "        indices = np.random.permutation(total_samples)  # Shuffle indices\n",
    "        \n",
    "        # Reorder the dataset based on shuffled indices\n",
    "        dataset = dataset.isel(batch=indices)    \n",
    "            \n",
    "        dataset = dataset.chunk({'batch': self.gpu_batch_size})\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51bd79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch : 0\n",
      "Batch : 1\n",
      "Batch : 2\n",
      "Batch : 3\n",
      "CPU times: user 19.8 s, sys: 6.4 s, total: 26.2 s\n",
      "Wall time: 9.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from wofscast.wofscast_task_config import DBZ_TASK_CONFIG\n",
    "\n",
    "n_epoches = 1\n",
    "\n",
    "directory = '/work/mflora/wofs-cast-data/datasets_jsons/2019'\n",
    "files = os.listdir(directory)[:128]\n",
    "paths = [os.path.join(directory, file) for file in files]\n",
    "\n",
    "generator = WoFSCastDataGenerator(DBZ_TASK_CONFIG, cpu_batch_size=128, gpu_batch_size=32)\n",
    "dataset = generator.open_mfdataset(paths)\n",
    "\n",
    "j=0\n",
    "for inputs, targets, forcings in generator(dataset):\n",
    "    print(f'Batch : {j}')\n",
    "    j+=1\n",
    "        \n",
    "\n",
    "# Dataset with 128 samples, prefetch_steps=4, batch_size=8\n",
    "#CPU times: user 20.9 s, sys: 8.99 s, total: 29.9 s\n",
    "#Wall time: 12.4 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e786b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

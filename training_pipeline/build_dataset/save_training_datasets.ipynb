{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24552a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "sys.path.append('/home/monte.flora/python_packages/frdd-wofs-cast/')\n",
    "\n",
    "from wofscast import data_utils\n",
    "from wofscast.wofscast_task_config import WOFS_TASK_CONFIG, train_lead_times\n",
    "from wofscast.data_generator import to_static_vars, add_local_solar_time, load_wofscast_data\n",
    "\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import dataclasses\n",
    "import random \n",
    "\n",
    "\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c4e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(dataset):\n",
    "\n",
    "    # Iterate through each variable in the Dataset\n",
    "    for var_name, data_array in dataset.items():\n",
    "        # Find boolean mask of NaNs\n",
    "        nan_mask = data_array.isnull()\n",
    "    \n",
    "        # Use np.where to find the indices of NaNs\n",
    "        nan_indices = np.where(nan_mask)\n",
    "    \n",
    "        # `nan_indices` is a tuple of arrays, each array corresponds to indices along one dimension\n",
    "        # Print the locations of NaNs\n",
    "        print(f\"NaN locations in {var_name}:\")\n",
    "        for dim, inds in zip(nan_mask.dims, nan_indices):\n",
    "            print(f\"  {dim}: {inds}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30be16c",
   "metadata": {},
   "source": [
    "### Randomly sample the different cases and ensemble members to improve training dataset diversity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5c2d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4860\n"
     ]
    }
   ],
   "source": [
    "data_paths = []\n",
    "# Only doing 2019 and 2020 and leaving 2021 as the evaluation dataset?\n",
    "for year in ['2019', '2020']:\n",
    "    data_paths.extend(glob(os.path.join(f'/work/mflora/wofs-cast-data/datasets/{year}/wrf*.nc')))\n",
    "\n",
    "# Function to parse date and ensemble member from a file name\n",
    "def parse_file_info(file_name):\n",
    "    parts = file_name.split('_')\n",
    "    date = parts[1]\n",
    "    ens_mem = parts[-1].split('.')[0]\n",
    "    return date, ens_mem\n",
    "\n",
    "# Organize files by date and ensemble member\n",
    "files_by_date_and_ens = defaultdict(lambda: defaultdict(list))\n",
    "for file_path in data_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    date, ens_mem = parse_file_info(file_name)\n",
    "    files_by_date_and_ens[date][ens_mem].append(file_path)\n",
    "\n",
    "# Decide how many samples you want per date and ensemble member\n",
    "samples_per_date_and_ens = 3  # Example: 1 sample per combination\n",
    "\n",
    "# Sample files\n",
    "sampled_files = []\n",
    "for date, ens_members in files_by_date_and_ens.items():\n",
    "    for ens_mem, files in ens_members.items():\n",
    "        if len(files) >= samples_per_date_and_ens:\n",
    "            sampled_files.extend(random.sample(files, samples_per_date_and_ens))\n",
    "        else:\n",
    "            sampled_files.extend(files)  # Add all if fewer files than desired samples\n",
    "\n",
    "# sampled_files now contains your randomly sampled files\n",
    "print(len(sampled_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7574c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def read_netcdfs(paths, dim, transform_func=None):\n",
    "    def process_one_path(path):\n",
    "        # use a context manager, to ensure the file gets closed after use\n",
    "        with xr.open_dataset(path) as ds:\n",
    "            # transform_func should do some sort of selection or\n",
    "            # aggregation\n",
    "            if transform_func is not None:\n",
    "                ds = transform_func(ds)\n",
    "            # load all data from the transformed dataset, to ensure we can\n",
    "            # use it after closing each original file\n",
    "            ds.load()\n",
    "            return ds\n",
    "        \n",
    "    datasets = [process_one_path(p) for p in tqdm(paths, desc=\"Processing files\")]\n",
    "    combined = xr.concat(datasets, dim)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cb98871",
   "metadata": {},
   "source": [
    "# Notes: \n",
    "\n",
    "I tried to load 4860 files (samples_per_date_and_ens = 3), but it too up too much memory and caused the \n",
    "system to crash. But 3000 files worked. Ran in about ~20 mins. Final combined file size is ~ 80 GBs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94caae9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c101d2fa4d4249e8abfac33474756a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing files:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 52s, sys: 1min 12s, total: 8min 4s\n",
      "Wall time: 20min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#inputs, targets, forcings = load_wofscast_data(sampled_files, \n",
    "#                                               train_lead_times, \n",
    "#                                               WOFS_TASK_CONFIG, client)\n",
    "\n",
    "# here we suppose we only care about the combined mean of each file;\n",
    "# you might also use indexing operations like .sel to subset datasets\n",
    "dataset = read_netcdfs(sampled_files[:3000], dim='batch',\n",
    "                        transform_func=add_local_solar_time)\n",
    "\n",
    "inputs, targets, forcings = data_utils.extract_inputs_targets_forcings(dataset,\n",
    "                                                        target_lead_times=train_lead_times,\n",
    "                                                        **dataclasses.asdict(WOFS_TASK_CONFIG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e494e898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Input Dims:  {'batch': 3000, 'time': 2, 'level': 17, 'lat': 150, 'lon': 150}\n",
      "Train Target Dims:  {'batch': 3000, 'time': 1, 'level': 17, 'lat': 150, 'lon': 150}\n",
      "Train Forcing Dims:  {'batch': 3000, 'time': 1, 'lon': 150, 'lat': 150}\n"
     ]
    }
   ],
   "source": [
    "print('Train Input Dims: ', inputs.dims.mapping)\n",
    "print('Train Target Dims: ', targets.dims.mapping)\n",
    "print('Train Forcing Dims: ', forcings.dims.mapping)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57cd9602",
   "metadata": {},
   "source": [
    "for ds in [inputs, targets, forcings]: \n",
    "    check_for_nans(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ad518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory Usage for Inputs, Targets, and Forcings: 81.98 GB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the memory usage in bytes\n",
    "memory_usage_bytes = targets.nbytes + inputs.nbytes + forcings.nbytes\n",
    "\n",
    "# Alternatively, for gigabytes (GB)\n",
    "memory_usage_gb = memory_usage_bytes / (1024**3)\n",
    "print(f\"Total Memory Usage for Inputs, Targets, and Forcings: {memory_usage_gb:.2f} GB\")\n",
    "\n",
    "\n",
    "# For the new reduce ~15-17 MB files, 500 files is 13.66 GBs\n",
    "\n",
    "# 1000 -> 27 \n",
    "# 2000 -> 54\n",
    "# 4000 -> 110\n",
    "# 5000 -> 135 GBs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90e8e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out_path = '/work/mflora/wofs-cast-data/train_datasets'\n",
    "inputs.to_netcdf(os.path.join(out_path, 'train_inputs.nc'))\n",
    "targets.to_netcdf(os.path.join(out_path, 'train_targets.nc'))\n",
    "forcings.to_netcdf(os.path.join(out_path, 'train_forcings.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8156249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7959291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

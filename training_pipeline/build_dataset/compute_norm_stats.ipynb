{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c70318a",
   "metadata": {},
   "source": [
    "## Compute the normalization statistics for the GraphCast code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec377e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import random \n",
    "import os\n",
    "\n",
    "import sys, os \n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from wofscast import graphcast_lam as graphcast\n",
    "import dask \n",
    "\n",
    "from wofscast.data_generator import (add_local_solar_time, \n",
    "                                     to_static_vars, \n",
    "                                     load_chunk, \n",
    "                                     dataset_to_input,\n",
    "                                     ZarrDataGenerator, \n",
    "                                     WRFZarrFileProcessor,\n",
    "                                     WoFSDataProcessor\n",
    "                                    )\n",
    "from wofscast import data_utils\n",
    "from wofscast.wofscast_task_config import (DBZ_TASK_CONFIG, \n",
    "                                           WOFS_TASK_CONFIG, \n",
    "                                           DBZ_TASK_CONFIG_1HR,\n",
    "                                           DBZ_TASK_CONFIG_FULL,\n",
    "                                           WOFS_TASK_CONFIG_1HR,\n",
    "                                           WOFS_TASK_CONFIG_5MIN, \n",
    "                                          )\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7678bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_subset(input_list, subset_size, seed=123):\n",
    "    \"\"\"\n",
    "    Get a random subset of a specified size from the input list.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_list : list\n",
    "        The original list from which to draw the subset.\n",
    "    subset_size : int\n",
    "        The size of the subset to be drawn.\n",
    "    seed : int, optional\n",
    "        The seed for the random number generator. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A random subset of the input list.\n",
    "    \"\"\"\n",
    "    if subset_size > len(input_list):\n",
    "        raise ValueError(\"subset_size must be less than or equal to the length of the input list\")\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    return random.sample(input_list, subset_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "505f2d2a",
   "metadata": {},
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def compute_normalization_stats(paths, gpu_batch_size, task_config, save_path, \n",
    "                                 preprocess_fn=None): \n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        \n",
    "        full_dataset = load_chunk(paths, \n",
    "                                  gpu_batch_size, preprocess_fn) \n",
    "\n",
    "        full_dataset = full_dataset.chunk({'lat' : 50, 'lon' : 50, 'batch' : 128, 'level' : 10})\n",
    "        \n",
    "        this_dataset = full_dataset.isel['time' = 0]\n",
    "        \n",
    "        # Setup computations using scattered data\n",
    "        mean_by_level = full_dataset.mean(dim=['time', 'lat', 'lon', 'batch'])\n",
    "        stddev_by_level = full_dataset.std(dim=['time', 'lat', 'lon', 'batch'], ddof=1)\n",
    "\n",
    "        time_diffs = full_dataset.diff(dim='time')\n",
    "        diffs_stddev_by_level = time_diffs.std(dim=['time', 'lat', 'lon', 'batch'], ddof=1)\n",
    "\n",
    "        # Save results to NetCDF files (this triggers the computation)\n",
    "        # Save results to NetCDF files (this triggers the computation)\n",
    "        with ProgressBar():\n",
    "            mean_by_level.to_netcdf(os.path.join(save_path, 'mean_by_level.nc'))\n",
    "            stddev_by_level.to_netcdf(os.path.join(save_path, 'stddev_by_level.nc'))\n",
    "            diffs_stddev_by_level.to_netcdf(os.path.join(save_path, 'diffs_stddev_by_level.nc'))\n",
    "\n",
    "        # Close all datasets\n",
    "        all_datasets = [full_dataset, mean_by_level, stddev_by_level, diffs_stddev_by_level]\n",
    "        \n",
    "        for ds in all_datasets:\n",
    "            ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bd9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.distributed import Client, progress\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def compute_normalization_stats(paths, gpu_batch_size, task_config, save_path, \n",
    "                                preprocess_fn=None, n_workers=4, threads_per_worker=1): \n",
    "    #client = Client(n_workers=n_workers, threads_per_worker=threads_per_worker)\n",
    "    #print(client)\n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        \n",
    "        # Load dataset in chunks\n",
    "        full_dataset = load_chunk(paths, gpu_batch_size, preprocess_fn) \n",
    "\n",
    "        # Re-chunk the dataset for efficient computation\n",
    "        full_dataset = full_dataset.chunk({'lat': 50, 'lon': 50, 'batch': 128})\n",
    "        \n",
    "        # Persist the dataset in memory to avoid reloading and recomputing\n",
    "        full_dataset = full_dataset.persist()\n",
    "        #progress(full_dataset)\n",
    "\n",
    "        # Compute mean and standard deviation by level\n",
    "        mean_by_level = full_dataset.mean(dim=['time', 'lat', 'lon', 'batch']).persist()\n",
    "        stddev_by_level = full_dataset.std(dim=['time', 'lat', 'lon', 'batch'], ddof=1).persist()\n",
    "\n",
    "        # Compute standard deviation of time differences\n",
    "        time_diffs = full_dataset.diff(dim='time')\n",
    "        diffs_stddev_by_level = time_diffs.std(dim=['time', 'lat', 'lon', 'batch'], ddof=1).persist()\n",
    "\n",
    "        # Save results to NetCDF files (this triggers the computation)\n",
    "        with ProgressBar():\n",
    "            mean_by_level.to_netcdf(os.path.join(save_path, 'mean_by_level.nc'))\n",
    "            stddev_by_level.to_netcdf(os.path.join(save_path, 'stddev_by_level.nc'))\n",
    "            diffs_stddev_by_level.to_netcdf(os.path.join(save_path, 'diffs_stddev_by_level.nc'))\n",
    "\n",
    "        # Close all datasets\n",
    "        all_datasets = [full_dataset, mean_by_level, stddev_by_level, diffs_stddev_by_level]\n",
    "        \n",
    "        for ds in all_datasets:\n",
    "            ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcaf665",
   "metadata": {},
   "source": [
    "### Compute the normalization statistics for the WOFS_TASK_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94e3374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n",
      "[########################################] | 100% Completed | 102.05 ms\n",
      "[########################################] | 100% Completed | 101.61 ms\n",
      "[########################################] | 100% Completed | 101.57 ms\n",
      "CPU times: user 4min 32s, sys: 2min 21s, total: 6min 53s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save to NetCDF files\n",
    "save_path = '/work/mflora/wofs-cast-data/norm_stats_5min/'\n",
    "\n",
    "#%%time\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "base_path = '/work/mflora/wofs-cast-data/datasets_5min'#_zarr'\n",
    "years = ['2019', '2020']\n",
    "\n",
    "def get_files_for_year(year):\n",
    "    year_path = join(base_path, year)\n",
    "    with os.scandir(year_path) as it:\n",
    "        return [join(year_path, entry.name) for entry in it \n",
    "                if entry.is_dir() and entry.name.endswith('ens_mem_09.zarr')]\n",
    "        #return [join(year_path, entry.name) for entry in it if entry.is_file()]\n",
    "    \n",
    "with ThreadPoolExecutor() as executor:\n",
    "    paths = []\n",
    "    for files in executor.map(get_files_for_year, years):\n",
    "        paths.extend(files)\n",
    "\n",
    "print(len(paths))\n",
    "\n",
    "random_paths = get_random_subset(paths, 512)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_fn(dataset):\n",
    "    latlon_path = '/work/mflora/wofs-cast-data/datasets_zarr/2021/wrfwof_2021-05-15_040000_to_2021-05-15_043000__10min__ens_mem_09.zarr'\n",
    "    preprocess = WoFSDataProcessor(latlon_path=latlon_path)\n",
    "    dataset = preprocess(dataset)\n",
    "    dataset = add_local_solar_time(dataset)\n",
    "    \n",
    "    dataset = dataset.drop_dims('datetime')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "compute_normalization_stats(random_paths, \n",
    "                            gpu_batch_size=len(random_paths), \n",
    "                            task_config=WOFS_TASK_CONFIG_5MIN, \n",
    "                            save_path=save_path, \n",
    "                           preprocess_fn=preprocess_fn)\n",
    "\n",
    "# 128 paths, gpu_batch=4 : 32s\n",
    "# 128 paths, gpu_batch=32 : 11.1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee64382",
   "metadata": {},
   "source": [
    "### Compute normalization statistics from DBZ_TASK_CONFIG_1HR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e6bf2c8",
   "metadata": {},
   "source": [
    "#%%time \n",
    "# Usage\n",
    "base_path = '/work2/wofs_zarr/'\n",
    "years = ['2019', '2020']\n",
    "resolution_minutes = 10\n",
    "\n",
    "# Specify the restrictions for testing\n",
    "restricted_dates = None\n",
    "restricted_times = ['1900', '2000', '2100', '2200', '2300', '0000', '0100', '0200', '0300']\n",
    "restricted_members = ['ENS_MEM_1', 'ENS_MEM_12', 'ENS_MEM_17', 'ENS_MEM_5']#, 'ENS_MEM_10', 'ENS_MEM_11']\n",
    "\n",
    "processor = WRFZarrFileProcessor(base_path, years, \n",
    "                             resolution_minutes, \n",
    "                             restricted_dates, \n",
    "                             restricted_times, restricted_members)\n",
    "\n",
    "paths = processor.run()\n",
    "\n",
    "random_paths = get_random_subset(paths, 6)\n",
    "\n",
    "save_path = '/work/mflora/wofs-cast-data/normalization_stats_full_domain'\n",
    "\n",
    "preprocessor = WoFSDataProcessor()\n",
    "\n",
    "compute_normalization_stats(random_paths, gpu_batch_size=len(random_paths), \n",
    "                            task_config=DBZ_TASK_CONFIG_FULL, \n",
    "                            save_path=save_path, batch_over_time=True, \n",
    "                            preprocess_fn=preprocessor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

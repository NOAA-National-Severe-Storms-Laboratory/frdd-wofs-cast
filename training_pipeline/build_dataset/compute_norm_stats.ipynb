{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c70318a",
   "metadata": {},
   "source": [
    "## Compute the normalization statistics for the GraphCast code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec377e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import random \n",
    "import os\n",
    "\n",
    "import sys, os \n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from wofscast import graphcast_lam as graphcast\n",
    "import dask \n",
    "\n",
    "from wofscast.data_generator import (add_local_solar_time, \n",
    "                                     to_static_vars, \n",
    "                                     load_chunk, \n",
    "                                     dataset_to_input,\n",
    "                                     ZarrDataGenerator, \n",
    "                                     WRFZarrFileProcessor,\n",
    "                                     WoFSDataProcessor\n",
    "                                    )\n",
    "from wofscast import data_utils\n",
    "from wofscast.wofscast_task_config import (DBZ_TASK_CONFIG, \n",
    "                                           WOFS_TASK_CONFIG, \n",
    "                                           DBZ_TASK_CONFIG_1HR,\n",
    "                                           DBZ_TASK_CONFIG_FULL\n",
    "                                          )\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7678bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_subset(input_list, subset_size, seed=123):\n",
    "    \"\"\"\n",
    "    Get a random subset of a specified size from the input list.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_list : list\n",
    "        The original list from which to draw the subset.\n",
    "    subset_size : int\n",
    "        The size of the subset to be drawn.\n",
    "    seed : int, optional\n",
    "        The seed for the random number generator. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A random subset of the input list.\n",
    "    \"\"\"\n",
    "    if subset_size > len(input_list):\n",
    "        raise ValueError(\"subset_size must be less than or equal to the length of the input list\")\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    return random.sample(input_list, subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba11ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def compute_normalization_stats(paths, gpu_batch_size, task_config, save_path, \n",
    "                                batch_over_time=False, preprocess_fn=None): \n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        \n",
    "        full_dataset = load_chunk(paths, batch_over_time, \n",
    "                                  gpu_batch_size, preprocess_fn) \n",
    "\n",
    "        full_dataset = full_dataset.chunk({'lat' : 50, 'lon' : 50, 'batch' : 128})\n",
    "        \n",
    "        # Setup computations using scattered data\n",
    "        mean_by_level = full_dataset.mean(dim=['time', 'lat', 'lon', 'batch'])\n",
    "        stddev_by_level = full_dataset.std(dim=['time', 'lat', 'lon', 'batch'], ddof=1)\n",
    "\n",
    "        time_diffs = full_dataset.diff(dim='time')\n",
    "        diffs_stddev_by_level = time_diffs.std(dim=['time', 'lat', 'lon', 'batch'], ddof=1)\n",
    "\n",
    "        # Save results to NetCDF files (this triggers the computation)\n",
    "        # Save results to NetCDF files (this triggers the computation)\n",
    "        with ProgressBar():\n",
    "            mean_by_level.to_netcdf(os.path.join(save_path, 'mean_by_level.nc'))\n",
    "            stddev_by_level.to_netcdf(os.path.join(save_path, 'stddev_by_level.nc'))\n",
    "            diffs_stddev_by_level.to_netcdf(os.path.join(save_path, 'diffs_stddev_by_level.nc'))\n",
    "\n",
    "        # Close all datasets\n",
    "        all_datasets = [full_dataset, mean_by_level, stddev_by_level, diffs_stddev_by_level]\n",
    "        \n",
    "        for ds in all_datasets:\n",
    "            ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcaf665",
   "metadata": {},
   "source": [
    "### Compute the normalization statistics for the WOFS_TASK_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3aaa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18053\n",
      "CPU times: user 26.5 ms, sys: 225 ms, total: 251 ms\n",
      "Wall time: 772 ms\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "base_path = '/work/mflora/wofs-cast-data/datasets_zarr'#_zarr'\n",
    "years = ['2019', '2020']\n",
    "\n",
    "def get_files_for_year(year):\n",
    "    year_path = join(base_path, year)\n",
    "    with os.scandir(year_path) as it:\n",
    "        return [join(year_path, entry.name) for entry in it if entry.is_dir() and entry.name.endswith('.zarr')]\n",
    "        #return [join(year_path, entry.name) for entry in it if entry.is_file()]\n",
    "    \n",
    "with ThreadPoolExecutor() as executor:\n",
    "    paths = []\n",
    "    for files in executor.map(get_files_for_year, years):\n",
    "        paths.extend(files)\n",
    "\n",
    "print(len(paths))\n",
    "\n",
    "random_paths = get_random_subset(paths, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638890db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 126.55 s\n",
      "[########################################] | 100% Completed | 30.83 s\n",
      "[########################################] | 100% Completed | 30.14 s\n",
      "CPU times: user 8min 25s, sys: 4min 57s, total: 13min 23s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "# Save to NetCDF files\n",
    "save_path = '/work/mflora/wofs-cast-data/test_norm_stats/'\n",
    "\n",
    "compute_normalization_stats(random_paths, \n",
    "                            gpu_batch_size=len(random_paths), \n",
    "                            task_config=WOFS_TASK_CONFIG, \n",
    "                            save_path=save_path)\n",
    "\n",
    "# 128 paths, gpu_batch=4 : 32s\n",
    "# 128 paths, gpu_batch=32 : 11.1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee64382",
   "metadata": {},
   "source": [
    "### Compute normalization statistics from DBZ_TASK_CONFIG_1HR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e6bf2c8",
   "metadata": {},
   "source": [
    "#%%time \n",
    "# Usage\n",
    "base_path = '/work2/wofs_zarr/'\n",
    "years = ['2019', '2020']\n",
    "resolution_minutes = 10\n",
    "\n",
    "# Specify the restrictions for testing\n",
    "restricted_dates = None\n",
    "restricted_times = ['1900', '2000', '2100', '2200', '2300', '0000', '0100', '0200', '0300']\n",
    "restricted_members = ['ENS_MEM_1', 'ENS_MEM_12', 'ENS_MEM_17', 'ENS_MEM_5']#, 'ENS_MEM_10', 'ENS_MEM_11']\n",
    "\n",
    "processor = WRFZarrFileProcessor(base_path, years, \n",
    "                             resolution_minutes, \n",
    "                             restricted_dates, \n",
    "                             restricted_times, restricted_members)\n",
    "\n",
    "paths = processor.run()\n",
    "\n",
    "random_paths = get_random_subset(paths, 6)\n",
    "\n",
    "save_path = '/work/mflora/wofs-cast-data/normalization_stats_full_domain'\n",
    "\n",
    "preprocessor = WoFSDataProcessor()\n",
    "\n",
    "compute_normalization_stats(random_paths, gpu_batch_size=len(random_paths), \n",
    "                            task_config=DBZ_TASK_CONFIG_FULL, \n",
    "                            save_path=save_path, batch_over_time=True, \n",
    "                            preprocess_fn=preprocessor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

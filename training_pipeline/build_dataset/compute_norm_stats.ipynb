{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c70318a",
   "metadata": {},
   "source": [
    "## Compute the normalization statistics for the GraphCast code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec377e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import random \n",
    "import os\n",
    "\n",
    "import sys, os \n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from wofscast import graphcast_lam as graphcast\n",
    "import dask \n",
    "\n",
    "from wofscast.data_generator import (add_local_solar_time, \n",
    "                                     to_static_vars, \n",
    "                                     load_chunk, \n",
    "                                     dataset_to_input,\n",
    "                                     ZarrDataGenerator, \n",
    "                                     WRFZarrFileProcessor,\n",
    "                                     WoFSDataProcessor\n",
    "                                    )\n",
    "from wofscast import data_utils\n",
    "from wofscast.wofscast_task_config import (DBZ_TASK_CONFIG, \n",
    "                                           WOFS_TASK_CONFIG, \n",
    "                                           DBZ_TASK_CONFIG_1HR,\n",
    "                                           DBZ_TASK_CONFIG_FULL\n",
    "                                          )\n",
    "from os.path import join\n",
    "\n",
    "# Save to NetCDF files\n",
    "save_path = '/work/mflora/wofs-cast-data/full_normalization_stats/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7678bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_subset(input_list, subset_size, seed=123):\n",
    "    \"\"\"\n",
    "    Get a random subset of a specified size from the input list.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_list : list\n",
    "        The original list from which to draw the subset.\n",
    "    subset_size : int\n",
    "        The size of the subset to be drawn.\n",
    "    seed : int, optional\n",
    "        The seed for the random number generator. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A random subset of the input list.\n",
    "    \"\"\"\n",
    "    if subset_size > len(input_list):\n",
    "        raise ValueError(\"subset_size must be less than or equal to the length of the input list\")\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    return random.sample(input_list, subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba11ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalization_stats(paths, gpu_batch_size, task_config, save_path, \n",
    "                                batch_over_time=False, preprocess_fn=None): \n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        full_dataset = load_chunk(paths, batch_over_time, \n",
    "                                  gpu_batch_size, preprocess_fn) \n",
    "\n",
    "        # Setup computations using scattered data\n",
    "        mean_by_level = full_dataset.mean(dim=['time', 'lat', 'lon', 'batch'])\n",
    "        stddev_by_level = full_dataset.std(dim=['time', 'lat', 'lon', 'batch'], ddof=1)\n",
    "\n",
    "        time_diffs = full_dataset.diff(dim='time')\n",
    "        diffs_stddev_by_level = time_diffs.std(dim=['time', 'lat', 'lon', 'batch'], ddof=1)\n",
    "\n",
    "        # Save results to NetCDF files (this triggers the computation)\n",
    "        mean_by_level.to_netcdf(os.path.join(save_path, 'mean_by_level.nc'))\n",
    "        stddev_by_level.to_netcdf(os.path.join(save_path, 'stddev_by_level.nc'))\n",
    "        diffs_stddev_by_level.to_netcdf(os.path.join(save_path, 'diffs_stddev_by_level.nc'))\n",
    "\n",
    "        # Close all datasets\n",
    "        all_datasets = [full_dataset, mean_by_level, stddev_by_level, diffs_stddev_by_level]\n",
    "        \n",
    "        for ds in all_datasets:\n",
    "            ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcaf665",
   "metadata": {},
   "source": [
    "### Compute the normalization statistics for the WOFS_TASK_CONFIG"
   ]
  },
  {
   "cell_type": "raw",
   "id": "216654b1",
   "metadata": {},
   "source": [
    "#Convert to Code to run\n",
    "\n",
    "%%time\n",
    "import os\n",
    "from os.path import join\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "base_path = '/work/mflora/wofs-cast-data/datasets_zarr'#_zarr'\n",
    "years = ['2019', '2020']\n",
    "\n",
    "def get_files_for_year(year):\n",
    "    year_path = join(base_path, year)\n",
    "    with os.scandir(year_path) as it:\n",
    "        return [join(year_path, entry.name) for entry in it if entry.is_dir() and entry.name.endswith('.zarr')]\n",
    "        #return [join(year_path, entry.name) for entry in it if entry.is_file()]\n",
    "    \n",
    "with ThreadPoolExecutor() as executor:\n",
    "    paths = []\n",
    "    for files in executor.map(get_files_for_year, years):\n",
    "        paths.extend(files)\n",
    "\n",
    "print(len(paths))\n",
    "\n",
    "#random_paths = get_random_subset(my_list, 4096)\n",
    "\n",
    "# Note: I did not use random paths for the full norm yet. \n",
    "\n",
    "compute_normalization_stats(random_paths, \n",
    "                            #paths[:4096], \n",
    "                            gpu_batch_size=32, \n",
    "                            task_config=WOFS_TASK_CONFIG, \n",
    "                            save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee64382",
   "metadata": {},
   "source": [
    "### Compute normalization statistics from DBZ_TASK_CONFIG_1HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab540f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 s, sys: 165 ms, total: 1.25 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Usage\n",
    "base_path = '/work2/wofs_zarr/'\n",
    "years = ['2019', '2020']\n",
    "resolution_minutes = 10\n",
    "\n",
    "# Specify the restrictions for testing\n",
    "restricted_dates = None\n",
    "restricted_times = ['1900', '2000', '2100', '2200', '2300', '0000', '0100', '0200', '0300']\n",
    "restricted_members = ['ENS_MEM_1', 'ENS_MEM_12', 'ENS_MEM_17', 'ENS_MEM_5']#, 'ENS_MEM_10', 'ENS_MEM_11']\n",
    "\n",
    "processor = WRFZarrFileProcessor(base_path, years, \n",
    "                             resolution_minutes, \n",
    "                             restricted_dates, \n",
    "                             restricted_times, restricted_members)\n",
    "\n",
    "paths = processor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b607fc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2516"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0919091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_paths = get_random_subset(paths, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3b772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 5s, sys: 1min 41s, total: 4min 47s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_path = '/work/mflora/wofs-cast-data/normalization_stats_full_domain'\n",
    "\n",
    "preprocessor = WoFSDataProcessor()\n",
    "\n",
    "compute_normalization_stats(random_paths, gpu_batch_size=32, \n",
    "                            task_config=DBZ_TASK_CONFIG_FULL, \n",
    "                            save_path=save_path, batch_over_time=True, \n",
    "                            preprocess_fn=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699e9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32914574",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "\n",
    "1. For a given ensemble member, combine WRFOUTs from forecast times into a single Zarr file\n",
    "    - Compute the add_local_solar_time during this phase? \n",
    "    - Store full 3D (all vertical levels and 5 min resolution)? \n",
    "\n",
    "- Store data for ensemble members separately? \n",
    "- Add dim for init times & ensemble members? \n",
    "- upscaled 1 km "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad3e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr \n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "from numcodecs import Blosc\n",
    "import dask \n",
    "import gc\n",
    "\n",
    "def read_netcdf(path, transform_func=None, chunks={}):\n",
    "    \"\"\"Read single NetCDF lazily and optionally apply a transform_func\"\"\"\n",
    "    dataset = xr.open_dataset(path, chunks=chunks, decode_times=False) \n",
    "    \n",
    "    if transform_func:\n",
    "        dataset = transform_func(dataset)\n",
    "\n",
    "    return dataset \n",
    "\n",
    "\n",
    "def read_mfnetcdfs_dask(paths, dim, transform_func=None, chunks={}, load=True):\n",
    "    \"\"\"Read multiple NetCDF files into memory, using Dask for parallel loading.\"\"\"\n",
    "    # Absolutely, crucial to set threads_per_worker=1!!!!\n",
    "    # https://forum.access-hive.org.au/t/netcdf-not-a-valid-id-errors/389/19\n",
    "    #To summarise in this thread, it looks like a work-around in netcdf4-python to deal \n",
    "    #with netcdf-c not being thread safe was removed in 1.6.1. \n",
    "    #The solution (for now) is to make sure your cluster only uses 1 thread per worker.\n",
    "\n",
    "    dataset = xr.open_mfdataset(paths, concat_dim=dim, combine='nested',\n",
    "                                parallel=True, preprocess=transform_func,\n",
    "                                chunks={}, decode_times=False) \n",
    "    \n",
    "    if load:\n",
    "        with ProgressBar():\n",
    "            loaded_dataset= dataset.compute()\n",
    "        return loaded_dataset\n",
    "\n",
    "    return dataset \n",
    "\n",
    "\n",
    "class WRFPreProcessor:\n",
    "    \"\"\"\n",
    "    WRFPreProcessor applies preprocessing steps to raw WRFOUT netcdf files. \n",
    "    The method utilizes dask's lazy loading feature in xarray. \n",
    "    \"\"\"\n",
    "    T_offset = 300. # Potential temperature offset for WRFOUT files. \n",
    "    \n",
    "    def __init__(self, \n",
    "                 variables,\n",
    "                 data_paths, \n",
    "                 time_resolution,\n",
    "                 destagger_mapper = \n",
    "                 {'U': 'west_east_stag', \n",
    "                  'V': 'south_north_stag',\n",
    "                  'W': 'bottom_top_stag',\n",
    "                  'GEOPOT': 'bottom_top_stag'\n",
    "                      }):\n",
    "        \n",
    "        self._variables = variables\n",
    "        self._destagger_mapper = destagger_mapper\n",
    "        self.time_resolution = time_resolution\n",
    "    \n",
    "        self.data_paths = data_paths\n",
    "    \n",
    "    def per_dataset(self, dataset):\n",
    "        preprocess_steps = [\n",
    "            'drop_variables', # Drop variables\n",
    "            'rename_coords', # Rename coords to match ERA5\n",
    "            'assign_lat_and_lon_coords', # Set Lat and Lon to 1d vectors. \n",
    "            'get_full_geopot', # Compute the full geopotential height field\n",
    "            'destagger', # Destagger the U,V,W,GEOPOT, etc\n",
    "            #'subset_vertical_levels', # Select the vertical layers to keep.\n",
    "            'remove_negative_water_vapor', # Set negative water vapor == 0.\n",
    "            'get_full_pot_temp', # Add the potential temperature offset\n",
    "        ]\n",
    "        for step in preprocess_steps:\n",
    "            dataset = getattr(self, step)(dataset)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def per_concat_dataset(self, dataset):\n",
    "        preprocess_steps = [\n",
    "            'rename_time_coord',\n",
    "            'add_time_dim', # Add TimeDelta dim to use GraphCast data utils\n",
    "            'unaccum_rainfall', # Convert accumulated rainfall to rain rate. \n",
    "        ]\n",
    "        for step in preprocess_steps:\n",
    "            dataset = getattr(self, step)(dataset)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def drop_variables(self, dataset):\n",
    "        all_vars = dataset.data_vars\n",
    "        drop_variables = [v for v in all_vars if v not in self._variables] \n",
    "        \n",
    "        return dataset.drop_vars(drop_variables+['XTIME'], \n",
    "                                errors='ignore')\n",
    "    \n",
    "    def get_full_geopot(self, dataset):\n",
    "        \"\"\"Combine the base and perturbation geopotential height\"\"\"\n",
    "         # Combine geopotential perturbation + base state\n",
    "        dataset['GEOPOT'] = dataset['PH'] + dataset['PHB']\n",
    "        dataset = dataset.drop_vars(['PH', 'PHB'])\n",
    "        \n",
    "        return dataset \n",
    "    \n",
    "    def get_full_pot_temp(self, dataset):\n",
    "        \"\"\"Add +300 K to the potential temperature field\"\"\"\n",
    "        if 'T' in dataset.data_vars:\n",
    "            dataset['T']+= self.T_offset\n",
    "            \n",
    "        return dataset\n",
    "        \n",
    "    def remove_negative_water_vapor(self, dataset):\n",
    "        \"\"\"Set negative water vapor == 0.\"\"\"\n",
    "        if 'QVAPOR' in dataset.data_vars: \n",
    "            dataset['QVAPOR'] = dataset['QVAPOR'].where(dataset['QVAPOR'] > 0, 0)\n",
    "\n",
    "        return dataset \n",
    "    \n",
    "    def rename_time_coord(self, dataset):\n",
    "        return dataset.rename({'Time': 'time'})\n",
    "    \n",
    "    def rename_coords(self, dataset):\n",
    "        \"\"\"Renaming coordinate variables to align with the ERA5 naming convention. \"\"\"\n",
    "        return dataset.rename({'bottom_top' :'level', \n",
    "                    #'XLAT': 'latitude', 'XLONG' : 'longitude', \n",
    "                    'south_north' : 'lat', 'west_east' : 'lon'\n",
    "               })\n",
    "    \n",
    "    def get_new_dim_name(self, stag_dim):\n",
    "        \"\"\" Rename the existing staggered coordinates to the destaggered name for consistency.\"\"\"\n",
    "        dim_name_map = {'west_east_stag': 'lon', 'south_north_stag': 'lat', 'bottom_top_stag': 'level'}\n",
    "        return dim_name_map.get(stag_dim, stag_dim)\n",
    "    \n",
    "    def destagger(self, dataset):\n",
    "        \"\"\"\n",
    "        General function to destagger any given variables along their specified dimensions.\n",
    "\n",
    "        Parameters:\n",
    "            dataset : xarray.Dataset\n",
    "                Input dataset.\n",
    "            destagger_mapper : dict\n",
    "                A mapping of variable names to their staggered dimensions.\n",
    "                For example: {'U': 'west_east_stag', 'V': 'south_north_stag'}\n",
    "\n",
    "        Returns:\n",
    "            dataset : xarray.Dataset\n",
    "            The dataset with destaggered variables.\n",
    "        \"\"\"\n",
    "        for var, stag_dim in self._destagger_mapper.items():\n",
    "            # Calculate the destaggered variable\n",
    "            destaggered_var = 0.5 * (dataset[var] + dataset[var].roll({stag_dim: -1}, roll_coords=False))\n",
    "            # Trim the last index of the staggered dimension\n",
    "            destaggered_var = destaggered_var.isel({stag_dim: slice(None, -1)})\n",
    "            # Rename the staggered dimension if a naming convention is provided\n",
    "            # This step can be customized or made optional based on specific requirements\n",
    "            new_dim_name = self.get_new_dim_name(stag_dim)  # Implement this method based on your context\n",
    "            destaggered_var = destaggered_var.rename({stag_dim: new_dim_name})\n",
    "            # Update the dataset with the destaggered variable\n",
    "            dataset[var] = destaggered_var\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def subset_vertical_levels(self, dataset):\n",
    "        # Subset the vertical levels (every N layers). \n",
    "        #TODO: Generalize this function!\n",
    "        return dataset.isel(level=dataset.level[::3].values)\n",
    "    \n",
    "    def assign_lat_and_lon_coords(self, dataset):\n",
    "        # Assign the 2D versions of 'xlat' and 'xlon' back to the dataset as coordinates\n",
    "        # Renaming coordinate variables to align with the ERA5 naming convention.\n",
    "        \n",
    "        # Latitude and longitude are expected to be 1d vectors. \n",
    "        lat_1d = dataset['XLAT'].isel(lon=0, Time=0)\n",
    "        lon_1d = dataset['XLONG'].isel(lat=0, Time=0)\n",
    "        \n",
    "        dataset = dataset.assign_coords(lat=lat_1d, lon=lon_1d)\n",
    "    \n",
    "        dataset = dataset.drop_vars(['XLAT', 'XLONG'])\n",
    "        \n",
    "        return dataset \n",
    "    \n",
    "    def add_time_dim(self, dataset):\n",
    "        \"\"\"Add time dimensions/coords to make use of GraphCast data utils\"\"\"\n",
    "        # Formating the time dimension for the graphcast code. \n",
    "        start_str = os.path.basename(self.data_paths[0]).split('_')[0] # wrfout or wrfwof \n",
    "\n",
    "        dts = [datetime.strptime(os.path.basename(f), f'{start_str}_d01_%Y-%m-%d_%H:%M:%S')\n",
    "               for f in self.data_paths]\n",
    "        time_range = [pd.Timestamp(dt) for dt in dts]\n",
    "\n",
    "        num_time_points = dataset.sizes['time']\n",
    "\n",
    "        dataset['time'] = time_range\n",
    "        \n",
    "        dataset = dataset.assign_coords(datetime=time_range)\n",
    "\n",
    "        # Convert 'time' dimension to timedeltas from the first time point\n",
    "        time_deltas = (dataset['time'] - dataset['time'][0]).astype('timedelta64[ns]')\n",
    "        dataset['time'] = time_deltas\n",
    "        \n",
    "        return dataset  \n",
    "    \n",
    "    def unaccum_rainfall(self, dataset):\n",
    "        \"\"\"\n",
    "        Calculate the difference in accumulated rainfall ('RAINNC') at each time step,\n",
    "        with an assumption that the first time step starts with zero rainfall.\n",
    "    \n",
    "        Parameters:\n",
    "        - ds: xarray.Dataset containing the 'RAINNC' variable\n",
    "    \n",
    "        Returns:\n",
    "            - Modified xarray.Dataset with the new variable 'RAINNC_DIFF'\n",
    "        \"\"\"\n",
    "        if 'RAINNC' not in self._variables:\n",
    "            return dataset \n",
    "        \n",
    "        # Calculate the difference along the time dimension\n",
    "        rain_diff = dataset['RAINNC'].diff(dim='time')\n",
    "    \n",
    "        # Prepend a zero for the first time step. This assumes that the difference\n",
    "        # for the first time step is zero since there's no previous time step to compare.\n",
    "        # We use np.concatenate to add the zero at the beginning. Ensure that the dimensions match.\n",
    "        # Adjust dimensions and coordinates according to your dataset's specific setup.\n",
    "        initial_zero = xr.zeros_like(dataset['RAINNC'].isel(time=0))\n",
    "        rain_diff_with_initial = xr.concat([initial_zero, rain_diff], dim='time')\n",
    "    \n",
    "        # Add the computed difference back to the dataset as a new variable\n",
    "        dataset['RAIN_AMOUNT'] = rain_diff_with_initial\n",
    "    \n",
    "        dataset = dataset.drop_vars(['RAINNC'])\n",
    "        \n",
    "        return dataset     \n",
    "    \n",
    "    \n",
    "import itertools \n",
    "from tqdm.notebook import tqdm \n",
    "from glob import glob\n",
    "import re\n",
    "    \n",
    "def filter_dates(dates, month_range = ['03', '04', '05', '06', '07']):\n",
    "    \"\"\"\n",
    "    Filter a list of dates to exclude January, February, November, and December. \n",
    "\n",
    "    Args:\n",
    "    - dates: A list of dates in 'YYYYMMDD' format.\n",
    "\n",
    "    Returns:\n",
    "    - A list of dates that fall in March - July \n",
    "    \"\"\"\n",
    "    filtered_dates = [date for date in dates if date[4:6] in month_range]\n",
    "    \n",
    "    return filtered_dates\n",
    "    \n",
    "\n",
    "class WRFFileGenerator: \n",
    "    \n",
    "    def __init__(self, duration_minutes='all', timestep_minutes=10, offset=0):\n",
    "        self.duration_minutes = duration_minutes\n",
    "        self.timestep_minutes = timestep_minutes \n",
    "        self.offset = offset \n",
    "    \n",
    "    def parse_filename_datetime(self, filename):\n",
    "        \"\"\"\n",
    "        Extract datetime object from a WRFOUT file path.\n",
    "    \n",
    "        Args:\n",
    "            filename (str): Filename in the format wrfwof_d01_YYYY-MM-DD_HH:MM:SS\n",
    "    \n",
    "        Returns:\n",
    "            datetime: Datetime object representing the timestamp in the filename.\n",
    "        \"\"\"\n",
    "        # Convert string to datetime object\n",
    "        return datetime.strptime(filename, 'wrfwof_d01_%Y-%m-%d_%H:%M:%S')\n",
    "    \n",
    "    def get_duration(self, directory_path):\n",
    "        init_time = os.path.basename(os.path.dirname(directory_path))\n",
    "        \n",
    "        ###print(init_time, init_time[-2:])\n",
    "        \n",
    "        if init_time[-2:] == '30':\n",
    "            return 180 # 3 hrs for bottom of the hour\n",
    "        else:\n",
    "            return 360 # 6 hrs for the top of the hour\n",
    "        \n",
    "    \n",
    "    def get_wrfwofs_files(self, directory_path):\n",
    "        \"\"\"\n",
    "        Load files for a given duration and timestep.\n",
    "    \n",
    "        Args:\n",
    "        directory_path (str): Path to the directory containing the files. \n",
    "\n",
    "        Returns:\n",
    "            list: List of filenames that match the given duration and timestep.\n",
    "        \"\"\"\n",
    "        # List all wrfwof files in the directory\n",
    "        files = glob(os.path.join(directory_path, 'wrfwof_d01_*'))\n",
    "        files.sort()\n",
    "                    \n",
    "        return files \n",
    "        \n",
    "    def file_path_generator(self, date_dir_path):\n",
    "        \"\"\"\n",
    "        List all directories matching the pattern /work2/wof/realtime/FCST/YYYY/YYYYMMDD/HHMM/ENS_MEM_NN/\n",
    "\n",
    "        Args:\n",
    "        base_path (str): Base directory to start the search (e.g., '/work2/wof/realtime/FCST/2019/')\n",
    "\n",
    "        Returns:\n",
    "        list: A list of all matching directory paths.\n",
    "        \"\"\"\n",
    "        ensemble_dirs = []\n",
    "        # Regular expression to match the date directories and ensemble member directories\n",
    "        date_pattern = re.compile(r\"\\d{8}$\")  # YYYYMMDD\n",
    "        time_pattern = re.compile(r\"\\d{4}$\")  # HHMM\n",
    "        ensemble_pattern = re.compile(r\"ENS_MEM_\\d{1,2}$\")  # ENS_MEM_N or ENS_MEM_NN\n",
    "\n",
    "        for root, dirs, files in os.walk(date_dir_path):\n",
    "            # Filter directories to continue the walk\n",
    "            dirs[:] = [d for d in dirs \n",
    "                       if date_pattern.match(d) or time_pattern.match(d) or ensemble_pattern.match(d)]\n",
    "        \n",
    "            # Check if any current directories are ensemble member directories\n",
    "            for dir_name in dirs:\n",
    "                if ensemble_pattern.match(dir_name):\n",
    "                    yield os.path.join(root, dir_name)\n",
    "    \n",
    "    def gen_file_paths(self, date_dir_paths):\n",
    "        for directory_path in date_dir_paths:\n",
    "            for path in self.file_path_generator(directory_path):\n",
    "                yield self.get_wrfwofs_files(path)\n",
    "                \n",
    "    def gen_single_paths(self, date_dir_paths):\n",
    "        for directory_path in date_dir_paths:\n",
    "            for path in self.file_path_generator(directory_path):\n",
    "                for p in self.get_wrfwofs_files(path):\n",
    "                    yield p            \n",
    "                \n",
    "             \n",
    "\n",
    "def get_file_path(files, \n",
    "                  dir_replace=('/work2/wof/realtime/FCST/', \n",
    "                               '/work2/wofs_zarr/')):\n",
    "    # Get the first path \n",
    "    path = files[0]\n",
    "    # Replace the /work2 path with the new dir\n",
    "    path = path.replace(dir_replace[0], dir_replace[1])\n",
    "    # Get the path and not the current filename\n",
    "    path = os.path.dirname(path)\n",
    "    \n",
    "    pattern = r'(ENS_MEM_)(\\d+)'\n",
    "\n",
    "    # Function to add leading zero if the number has less than 2 digits\n",
    "    def add_leading_zero(match):\n",
    "        ens_mem = match.group(1)  # The 'ENS_MEM_' part\n",
    "        number = match.group(2)   # The number part\n",
    "        return f'{ens_mem}{int(number):02}'  # Format number with leading zero if necessary\n",
    "\n",
    "    # Replace the found pattern in the path using the add_leading_zero function\n",
    "    new_path = re.sub(pattern, add_leading_zero, path)\n",
    "    \n",
    "    return new_path\n",
    "    \n",
    "def create_filename_from_list(file_paths, time_resolution):\n",
    "    \"\"\"\n",
    "    Create a filename based on the first and last elements of a list of file paths.\n",
    "\n",
    "    Args:\n",
    "            file_paths (list): A list of file paths.\n",
    "\n",
    "    Returns:\n",
    "            str: A string representing the generated filename, which includes the start and end datetime.\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        return \"No files provided\"\n",
    "\n",
    "    # Extract start time from the first element\n",
    "    start_time = os.path.basename(file_paths[0]).replace('wrfwof_d01_', '')  \n",
    "    # Extract end time from the last element\n",
    "    end_time = os.path.basename(file_paths[-1]).replace('wrfwof_d01_', '')  \n",
    "    \n",
    "    # Format the filename\n",
    "    ens_mem = os.path.basename(os.path.dirname(file_paths[-1])).split('_')[-1]\n",
    "    \n",
    "    filename = f\"wrfwof_{start_time}_to_{end_time}_{time_resolution}.zarr\"\n",
    "\n",
    "    # Cleaning up the datetime format to remove colons and make it filesystem-friendly\n",
    "    for char in [\":\"]:\n",
    "        filename = filename.replace(char, \"\")\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2f1fdf7",
   "metadata": {},
   "source": [
    "def _process_files(data_paths, variables):\n",
    "    \"\"\"Process a single set of WRFOUT files and converting them into Zarr\"\"\"\n",
    "    time_resolution=f'{timestep_minutes}min'\n",
    "    \n",
    "    preprocessor = WRFPreProcessor(variables, \n",
    "                 data_paths, time_resolution=time_resolution)\n",
    "\n",
    "    if len(data_paths) == 0:\n",
    "        return f'Could not process!'\n",
    "    \n",
    "    dataset = read_mfnetcdfs_dask(data_paths, dim='Time', \n",
    "                              transform_func=preprocessor.per_dataset, \n",
    "                              chunks={}, load=False)\n",
    "\n",
    "    dataset = preprocessor.per_concat_dataset(dataset)\n",
    "    \n",
    "    # Write out to Zarr. \n",
    "    compressor = Blosc(cname='zstd', \n",
    "                       clevel=7, \n",
    "                       shuffle=Blosc.BITSHUFFLE)\n",
    "    \n",
    "    dir_path = get_file_path(data_paths)\n",
    "    \n",
    "    out_path = os.path.join(dir_path, \n",
    "                            create_filename_from_list(data_paths, time_resolution))\n",
    "    \n",
    "    if not os.path.exists(dir_path):\n",
    "        # Create the directory, do not raise an error if it already exists\n",
    "        print(f'Creating {dir_path}')\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    print(f'Saving {out_path}...')\n",
    "    \n",
    "    # Rechunk for the Zarr save file (increases run time, \n",
    "    # but could be useful for loading in the future) \n",
    "    dataset = dataset.chunk({'time' : 1})\n",
    "    \n",
    "    dataset.to_zarr(out_path, \n",
    "                    mode='w', \n",
    "                    consolidated=True, \n",
    "                    encoding={var: {'compressor': compressor} for var in dataset.variables})\n",
    "    \n",
    "    dataset.close() \n",
    "    \n",
    "    del dataset\n",
    "    gc.collect() \n",
    "    \n",
    "    return f'Processed {out_path}!'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_base_path(path, new_base_path, old_base_path='/work2/wof/realtime/FCST/'):\n",
    "    new_path = path.replace(old_base_path, new_base_path)\n",
    "    return new_path \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4453bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def _process_one_file(path, variables, timestep_minutes):\n",
    "    \"\"\"Process a single set of WRFOUT files and convert them into Zarr.\"\"\"\n",
    "    try:\n",
    "        # Validate the input parameters\n",
    "        if not isinstance(path, str) or not os.path.exists(path):\n",
    "            raise ValueError(f\"Invalid file path provided: {path}\")\n",
    "\n",
    "        if not isinstance(variables, list) or not variables:\n",
    "            raise ValueError(\"Variables must be a non-empty list\")\n",
    "\n",
    "        if not isinstance(timestep_minutes, int) or timestep_minutes <= 0:\n",
    "            raise ValueError(f\"Invalid timestep: {timestep_minutes}\")\n",
    "\n",
    "        # Set the time resolution\n",
    "        time_resolution = f'{timestep_minutes}min'\n",
    "        \n",
    "        # Initialize the preprocessor\n",
    "        preprocessor = WRFPreProcessor(variables, path, time_resolution=time_resolution)\n",
    "\n",
    "        # Read the NetCDF file using the specified preprocessor\n",
    "        dataset = read_netcdf(path, transform_func=preprocessor.per_dataset, chunks={})\n",
    "\n",
    "        # Replace base path and set the output path\n",
    "        out_path = replace_base_path(path,\n",
    "                                     new_base_path='/work2/wofs_zarr/', \n",
    "                                     old_base_path='/work2/wof/realtime/FCST/')\n",
    "        out_path = out_path + '.zarr'\n",
    "        \n",
    "        # Configure the Blosc compressor\n",
    "        compressor = Blosc(cname='zstd', clevel=7, shuffle=Blosc.BITSHUFFLE)\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(out_path)\n",
    "        if not os.path.exists(output_dir):\n",
    "            print(f'Creating directory {output_dir}...')\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the dataset to a Zarr file\n",
    "        print(f'Saving to {out_path}...')\n",
    "        dataset.to_zarr(out_path, mode='w', \n",
    "                        encoding={var: {'compressor': compressor} for var in dataset.variables})\n",
    "\n",
    "        # Clean up\n",
    "        dataset.close()\n",
    "        del dataset\n",
    "        gc.collect()\n",
    "\n",
    "        return f'Processed {out_path} successfully!'\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing file {path}: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_message)\n",
    "        return error_message"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a85a3b7",
   "metadata": {},
   "source": [
    "%%time \n",
    "# FOR TESTING A SINGLE CASE. GOOD FOR DEBUGGING!\n",
    "\n",
    "from glob import glob\n",
    "from dask.distributed import Client, progress\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "\n",
    "files = glob('/work2/wof/realtime/FCST/2019/20190503/0130/ENS_MEM_1/wrfwof*')\n",
    "files.sort()\n",
    "\n",
    "timestep_minutes = 5\n",
    "\n",
    "with Client(threads_per_worker=1) as client:\n",
    "    ds = dask.compute(_process_files(files, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d15057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "VARS_3D_TO_KEEP = ['U', 'V', 'W', 'T', 'PH', 'PHB', 'QVAPOR']\n",
    "VARS_2D_TO_KEEP = ['T2', 'RAINNC', 'COMPOSITE_REFL_10CM', 'UP_HELI_MAX', \n",
    "                   'Q2', 'U10', 'V10', 'REL_VORT_MAX', 'SWDOWN', 'WSPD80', \n",
    "                   'W_UP_MAX', 'LWP'\n",
    "                  ]\n",
    "\n",
    "CONSTANTS = ['HGT', 'XLAND']\n",
    "\n",
    "variables = VARS_3D_TO_KEEP + VARS_2D_TO_KEEP + CONSTANTS\n",
    "\n",
    "\n",
    "all_date_dir_paths = [] \n",
    "years = ['2020']#, '2021']\n",
    "for year in years:\n",
    "    base_path = os.path.join('/work2/wof/realtime/FCST/', year)\n",
    "    all_dates = os.listdir(base_path)\n",
    "    all_dates.sort()\n",
    "    \n",
    "    dates = filter_dates(all_dates)\n",
    "    all_date_dir_paths.extend([os.path.join(base_path, d) for d in dates[:1]])\n",
    "    \n",
    "all_date_dir_paths = ['/work2/wof/realtime/FCST/2019/20190506']\n",
    "    \n",
    "timestep_minutes = 5\n",
    "offset = 0\n",
    "\n",
    "generator = WRFFileGenerator(duration_minutes='all',\n",
    "    timestep_minutes=timestep_minutes, offset=offset)\n",
    "\n",
    "# Need a correct number for the progress bar? \n",
    "file_lists = list(generator.gen_single_paths(all_date_dir_paths))\n",
    "\n",
    "#file_lists = file_lists[:5]\n",
    "\n",
    "# Create a list of delayed objects\n",
    "#tasks = [_process_one_file(path, variables, timestep_minutes) for path in file_lists]\n",
    "\n",
    "#with ProgressBar():\n",
    "#    results = dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e8708b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/work2/wof/realtime/FCST/2019/20190506']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date_dir_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "998fb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lists.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7749afa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_lists)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd14fbd8",
   "metadata": {},
   "source": [
    "%%time \n",
    "\n",
    "from dask.distributed import Client, progress\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "VARS_3D_TO_KEEP = ['U', 'V', 'W', 'T', 'PH', 'PHB', 'QVAPOR']\n",
    "VARS_2D_TO_KEEP = ['T2', 'RAINNC', 'COMPOSITE_REFL_10CM', 'UP_HELI_MAX', \n",
    "                   'Q2', 'U10', 'V10', 'REL_VORT_MAX', 'SWDOWN', 'WSPD80', \n",
    "                   'W_UP_MAX', 'LWP'\n",
    "                  ]\n",
    "\n",
    "CONSTANTS = ['HGT', 'XLAND']\n",
    "\n",
    "variables = VARS_3D_TO_KEEP + VARS_2D_TO_KEEP + CONSTANTS\n",
    "\n",
    "\n",
    "all_date_dir_paths = [] \n",
    "years = ['2020']#, '2021']\n",
    "for year in years:\n",
    "    base_path = os.path.join('/work2/wof/realtime/FCST/', year)\n",
    "    all_dates = os.listdir(base_path)\n",
    "    all_dates.sort()\n",
    "    \n",
    "    dates = filter_dates(all_dates)\n",
    "    all_date_dir_paths.extend([os.path.join(base_path, d) for d in dates[:1]])\n",
    "    \n",
    "timestep_minutes = 5\n",
    "offset = 0\n",
    "\n",
    "generator = WRFFileGenerator(duration_minutes='all',\n",
    "    timestep_minutes=timestep_minutes, offset=offset)\n",
    "\n",
    "# Need a correct number for the progress bar? \n",
    "file_lists = list(generator.gen_single_paths(all_date_dir_paths))\n",
    "\n",
    "file_lists = file_lists[:5]\n",
    "\n",
    "with Client(threads_per_worker=1) as client:\n",
    "    tasks = [_process_one_file(files, variables) \n",
    "           for files in file_lists]\n",
    "    \n",
    "    futures = client.compute(tasks)\n",
    "    with ProgressBar():\n",
    "        results = client.gather(futures)\n",
    "    print(\"Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e17162",
   "metadata": {},
   "source": [
    "## Step 1. Generate a Kerchunk JSON for the NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49dba99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 12.6 s, total: 14.8 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import os\n",
    "from dask import delayed\n",
    "import dask\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "from wofscast.data_generator import add_local_solar_time\n",
    "import wofscast.my_graphcast as graphcast\n",
    "from wofscast import data_utils\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "import glob\n",
    "import ujson\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "\n",
    "\n",
    "def ensure_json_ext(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure the given filename ends with '.json' and remove any other extensions.\n",
    "    If the filename does not have '.json', append '.json' to the root name.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The original filename.\n",
    "\n",
    "    Returns:\n",
    "        str: The filename with only a '.json' extension.\n",
    "    \"\"\"\n",
    "    # Split the filename to remove its existing extension (if any)\n",
    "    root_name, _ = os.path.splitext(filename)\n",
    "    \n",
    "    # Add `.json` as the extension\n",
    "    return f\"{root_name}.json\"\n",
    "\n",
    "def gen_json(u, output_dir=\"/work/mflora/wofs-cast-data/jsons/\", \n",
    "             original_dir = '/work2/wof/realtime/FCST/'):\n",
    "\n",
    "    # File system options\n",
    "    so = dict(\n",
    "        mode=\"rb\", anon=True, default_fill_cache=False,\n",
    "        default_cache_type=\"none\"\n",
    "    )\n",
    "\n",
    "    # Open the NetCDF file and generate JSON\n",
    "    try:\n",
    "        with fsspec.open(u, **so) as inf:\n",
    "            h5chunks = SingleHdf5ToZarr(inf, u, inline_threshold=300)\n",
    "            output_path = ensure_json_ext(u.replace(original_dir, output_dir))\n",
    "            # Ensure output directory exists\n",
    "            if not os.path.exists(os.path.dirname(output_path)):\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            with open(output_path, 'wb') as outf:\n",
    "                outf.write(ujson.dumps(h5chunks.translate()).encode())\n",
    "            return f\"Generated JSON for {output_path}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate JSON for {u}: {e}\")\n",
    "\n",
    "\n",
    "def open_mfdataset_batch(paths, batch_chunk_size, concat_dim='batch'):\n",
    "    \"\"\"Using kerchunking, individual zarr or netcdf files are represented by individual json files. \n",
    "    We can then trick xarray into believing the jsons are individual zarr files and treat them as\n",
    "    one file. This function uses dask.delayed to lazily load the individual jsons in parallel and \n",
    "    then re-chunks based on the batch chunk size for efficiently batch loading. \n",
    "    \n",
    "    paths : list of paths : paths to a set of forecasts for a given ensemble member \n",
    "    \n",
    "    \"\"\"\n",
    "    @delayed\n",
    "    def load_dataset_from_json(json_path):\n",
    "        \"\"\"Load a dataset from a Kerchunk JSON descriptor.\"\"\"\n",
    "        # Using fsspec to create a mapper from the JSON reference\n",
    "        mapper = fsspec.get_mapper('reference://', fo=json_path, remote_protocol='file')\n",
    "        # Load the dataset using xarray with the Zarr engine\n",
    "        ds = xr.open_dataset(mapper, engine='zarr', consolidated=False, chunks={}, decode_times=False)\n",
    "        \n",
    "        ##ds = add_local_solar_time(ds)\n",
    "        \n",
    "        return ds\n",
    "\n",
    "    def load_and_concatenate(json_files, concat_dim='batch'):\n",
    "        \"\"\"Load multiple datasets from JSON files and concatenate them along a specified dimension.\"\"\"\n",
    "        # Load each dataset using Dask delayed and collect them in a list\n",
    "        datasets = [load_dataset_from_json(json_file) for json_file in json_files]\n",
    "    \n",
    "        # Use Dask to compute the list of datasets\n",
    "        datasets = dask.compute(*datasets)\n",
    "    \n",
    "        # Concatenate all datasets along the specified dimension\n",
    "        combined_dataset = xr.concat(datasets, dim=concat_dim)\n",
    "    \n",
    "        for ds in datasets:\n",
    "            ds.close() \n",
    "    \n",
    "        return combined_dataset\n",
    "\n",
    "\n",
    "    datasets_per_time = [load_and_concatenate(p, \n",
    "                                     concat_dim='Time') for p in paths] \n",
    "    \n",
    "    \n",
    "    dataset = xr.concat(datasets_per_time, dim='batch')\n",
    "    \n",
    "    dataset = dataset.rename({'Time' : 'time'})\n",
    "    \n",
    "    dataset = dataset.chunk({'batch': batch_chunk_size})\n",
    "    \n",
    "    return dataset \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3147b96",
   "metadata": {},
   "source": [
    "%%time\n",
    "paths1 = glob.glob('/work2/wof/realtime/FCST/2021/20210409/0200/ENS_MEM_01/wrfwof*')\n",
    "paths2 = glob.glob('/work2/wof/realtime/FCST/2021/20210409/0200/ENS_MEM_02/wrfwof*')\n",
    "\n",
    "paths1.sort()\n",
    "paths2.sort()\n",
    "\n",
    "paths1 = paths1[:12]\n",
    "paths2 = paths2[:12]\n",
    "\n",
    "#results = dask.compute(*[dask.delayed(gen_json)(u) for u in paths1+paths2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24bf409",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths1 = glob.glob('/work/mflora/wofs-cast-data/jsons/2021/20210409/0200/ENS_MEM_01/wrfwof*')\n",
    "paths2 = glob.glob('/work/mflora/wofs-cast-data/jsons/2021/20210409/0200/ENS_MEM_01/wrfwof*')\n",
    "\n",
    "paths1.sort()\n",
    "paths2.sort()\n",
    "\n",
    "paths = [paths1, paths2]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d89a10a",
   "metadata": {},
   "source": [
    "%%time \n",
    "dataset = open_mfdataset_batch(paths, \n",
    "                               batch_chunk_size=8, concat_dim='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "915e7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# the number of gridpoints in one direction; square domain.\n",
    "DOMAIN_SIZE = 300\n",
    "\n",
    "VARS_3D = ['U', 'V', 'W', 'T', 'GEOPOT', 'QVAPOR']\n",
    "VARS_2D = ['T2', 'COMPOSITE_REFL_10CM', 'UP_HELI_MAX']\n",
    "STATIC_VARS = ['XLAND', 'HGT']\n",
    "\n",
    "INPUT_VARS = VARS_3D + VARS_2D + STATIC_VARS\n",
    "TARGET_VARS = VARS_3D + VARS_2D\n",
    "\n",
    "# I compute this myself rather than using the GraphCast code. \n",
    "FORCING_VARS = (\n",
    "            'SWDOWN'\n",
    "        )\n",
    "\n",
    "# Not pressure levels, but just vertical array indices at the moment. \n",
    "# When I created the wrfwof files, I pre-sampled every 3 levels. \n",
    "PRESSURE_LEVELS = np.arange(50)\n",
    "\n",
    "# Loads data from the past 20 minutes (2 steps) and \n",
    "# creates a target over the next 10-60 min. \n",
    "INPUT_DURATION = '10min'\n",
    "train_lead_times = '5min'\n",
    "\n",
    "task_config = graphcast.TaskConfig(\n",
    "      input_variables=INPUT_VARS,\n",
    "      target_variables=TARGET_VARS,\n",
    "      forcing_variables=FORCING_VARS,\n",
    "      pressure_levels=PRESSURE_LEVELS,\n",
    "      input_duration=INPUT_DURATION,\n",
    "      n_vars_2D = len(VARS_2D),\n",
    "      domain_size = DOMAIN_SIZE,\n",
    "      tiling=None\n",
    " )\n",
    "\n",
    "def wofscast_data_generator(paths, \n",
    "                            train_lead_times, \n",
    "                            task_config,\n",
    "                            batch_chunk_size=256, \n",
    "                            client=None,):\n",
    "    \n",
    "    with open_mfdataset_batch(paths, batch_chunk_size) as ds:\n",
    "   \n",
    "            total_samples = len(ds.batch)\n",
    "            total_batches = total_samples // batch_chunk_size + (1 if total_samples % batch_chunk_size > 0 else 0)\n",
    "    \n",
    "            for batch_num in tqdm(range(total_batches), desc='Loading Zarr Batch..'):\n",
    "                start_idx = batch_num * batch_chunk_size\n",
    "                end_idx = min((batch_num + 1) * batch_chunk_size, total_samples)\n",
    "                batch_indices = slice(start_idx, end_idx)  # Use slice for more efficient indexing\n",
    "        \n",
    "                # Load this batch into memory. \n",
    "                this_batch = ds.isel(batch=batch_indices)\n",
    "        \n",
    "                inputs, targets, forcings = data_utils.batch_extract_inputs_targets_forcings(\n",
    "                    this_batch,\n",
    "                    n_input_steps=2,\n",
    "                    n_target_steps=1,\n",
    "                    target_lead_times=train_lead_times,\n",
    "                    **dataclasses.asdict(task_config)\n",
    "                )\n",
    "        \n",
    "                inputs = to_static_vars(inputs)\n",
    "        \n",
    "                inputs = inputs.transpose('batch', 'time', 'lat', 'lon', 'level')\n",
    "                targets = targets.transpose('batch', 'time', 'lat', 'lon', 'level')\n",
    "                forcings = forcings.transpose('batch', 'time', 'lat', 'lon')\n",
    "\n",
    "                inputs, targets, forcings = dask.compute(inputs, targets, forcings)\n",
    "            \n",
    "                yield inputs, targets, forcings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "197e5bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Zarr Batch..:   0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_input, batch_target, batch_forcings \u001b[38;5;129;01min\u001b[39;00m wofscast_data_generator(paths, \n\u001b[1;32m      2\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5min\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m                             task_config,\n\u001b[1;32m      4\u001b[0m                             batch_chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[1;32m      5\u001b[0m                             client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m, in \u001b[0;36mwofscast_data_generator\u001b[0;34m(paths, train_lead_times, task_config, batch_chunk_size, client)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Load this batch into memory. \u001b[39;00m\n\u001b[1;32m     56\u001b[0m this_batch \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39misel(batch\u001b[38;5;241m=\u001b[39mbatch_indices)\n\u001b[0;32m---> 58\u001b[0m inputs, targets, forcings \u001b[38;5;241m=\u001b[39m \u001b[43mdata_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_extract_inputs_targets_forcings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_input_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_target_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_lead_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_lead_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m inputs \u001b[38;5;241m=\u001b[39m to_static_vars(inputs)\n\u001b[1;32m     68\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/python_packages/frdd-wofs-cast/wofscast/data_utils.py:477\u001b[0m, in \u001b[0;36mbatch_extract_inputs_targets_forcings\u001b[0;34m(dataset, n_input_steps, n_target_steps, input_variables, target_variables, forcing_variables, pressure_levels, input_duration, target_lead_times, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m n_total_steps\u001b[38;5;241m=\u001b[39mn_input_steps\u001b[38;5;241m+\u001b[39mn_target_steps \u001b[38;5;66;03m# 2 input steps + 1 target step\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mtime\u001b[38;5;241m.\u001b[39msize\u001b[38;5;241m-\u001b[39mn_total_steps, n_total_steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 477\u001b[0m     _inputs, _targets, _forcings \u001b[38;5;241m=\u001b[39m \u001b[43mextract_inputs_targets_forcings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mn_total_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#, datetime=slice(i,i+n_total_steps)), \u001b[39;49;00m\n\u001b[1;32m    479\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_lead_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_lead_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_variables\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforcing_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforcing_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpressure_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpressure_levels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_duration\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(_inputs)\n\u001b[1;32m    487\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(_targets)\n",
      "File \u001b[0;32m~/python_packages/frdd-wofs-cast/wofscast/data_utils.py:434\u001b[0m, in \u001b[0;36mextract_inputs_targets_forcings\u001b[0;34m(dataset, input_variables, target_variables, forcing_variables, pressure_levels, input_duration, target_lead_times, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Deprecated part of the original GraphCast code.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# \"Forcings\" include derived variables that do not exist in the original ERA5\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# or HRES datasets, as well as other variables (e.g. tisr) that need to be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `datetime` is needed by add_derived_vars but breaks autoregressive rollouts.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdrop_vars(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 434\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[43mextract_input_target_times\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_duration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_lead_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_lead_times\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(forcing_variables) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(target_variables):\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForcing variables \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforcing_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverlap with target variables \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m   )\n",
      "File \u001b[0;32m~/python_packages/frdd-wofs-cast/wofscast/data_utils.py:361\u001b[0m, in \u001b[0;36mextract_input_target_times\u001b[0;34m(dataset, input_duration, target_lead_times)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Shift the coordinates for the time axis so that a timedelta of zero\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# corresponds to the forecast reference time. That is, the final timestep\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# that's available as input to the forecast, with all following timesteps\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# forming the target period which needs to be predicted.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# This means the time coordinates are now forecast lead times.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m time \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mcoords[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 361\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39massign_coords(time\u001b[38;5;241m=\u001b[39m\u001b[43mtime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_duration\u001b[49m \u001b[38;5;241m-\u001b[39m time[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Slice out targets:\u001b[39;00m\n\u001b[1;32m    364\u001b[0m targets \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39msel({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_lead_times})\n",
      "File \u001b[0;32m/work/mflora/miniconda3/envs/wofs-cast/lib/python3.10/site-packages/xarray/core/_typed_ops.py:222\u001b[0m, in \u001b[0;36mDataArrayOpsMixin.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/mflora/miniconda3/envs/wofs-cast/lib/python3.10/site-packages/xarray/core/dataarray.py:4620\u001b[0m, in \u001b[0;36mDataArray._binary_op\u001b[0;34m(self, other, f, reflexive)\u001b[0m\n\u001b[1;32m   4616\u001b[0m other_variable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m\"\u001b[39m, other)\n\u001b[1;32m   4617\u001b[0m other_coords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   4619\u001b[0m variable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 4620\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_variable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reflexive\n\u001b[1;32m   4622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m f(other_variable, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable)\n\u001b[1;32m   4623\u001b[0m )\n\u001b[1;32m   4624\u001b[0m coords, indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39m_merge_raw(other_coords, reflexive)\n\u001b[1;32m   4625\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_name(other)\n",
      "File \u001b[0;32m/work/mflora/miniconda3/envs/wofs-cast/lib/python3.10/site-packages/xarray/core/_typed_ops.py:428\u001b[0m, in \u001b[0;36mVariableOpsMixin.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/mflora/miniconda3/envs/wofs-cast/lib/python3.10/site-packages/xarray/core/variable.py:2705\u001b[0m, in \u001b[0;36mVariable._binary_op\u001b[0;34m(self, other, f, reflexive)\u001b[0m\n\u001b[1;32m   2702\u001b[0m attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;28;01mif\u001b[39;00m keep_attrs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2704\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2705\u001b[0m         \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reflexive \u001b[38;5;28;01melse\u001b[39;00m f(other_data, self_data)\n\u001b[1;32m   2706\u001b[0m     )\n\u001b[1;32m   2707\u001b[0m result \u001b[38;5;241m=\u001b[39m Variable(dims, new_data, attrs\u001b[38;5;241m=\u001b[39mattrs)\n\u001b[1;32m   2708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations."
     ]
    }
   ],
   "source": [
    "for batch_input, batch_target, batch_forcings in wofscast_data_generator(paths, \n",
    "                            '5min', \n",
    "                            task_config,\n",
    "                            batch_chunk_size=8, \n",
    "                            client=None,):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df299ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

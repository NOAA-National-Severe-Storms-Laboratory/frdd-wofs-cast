{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import sys, os \n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "# Set JAX_TRACEBACK_FILTERING to off for detailed traceback\n",
    "#os.environ['JAX_TRACEBACK_FILTERING'] = 'on'\n",
    "\n",
    "\n",
    "# @title Imports\n",
    "import dataclasses\n",
    "import datetime\n",
    "import functools\n",
    "import math\n",
    "import re\n",
    "from typing import Optional\n",
    "from glob import glob\n",
    "import gc\n",
    "import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "#from google.cloud import storage\n",
    "from wofscast import autoregressive #_lam as autoregressive\n",
    "from wofscast import casting\n",
    "from wofscast import checkpoint\n",
    "from wofscast import data_utils\n",
    "from wofscast import gencast_lam as gencast\n",
    "from wofscast import normalization\n",
    "from wofscast import rollout\n",
    "from wofscast import xarray_jax\n",
    "from wofscast import xarray_tree\n",
    "from wofscast.data_generator import (ZarrDataGenerator, \n",
    "                                     add_local_solar_time, \n",
    "                                     to_static_vars, \n",
    "                                     open_zarr,\n",
    "                                     dataset_to_input\n",
    "                                    )\n",
    "\n",
    "from wofscast.wofscast_task_config import DBZ_TASK_CONFIG, WOFS_TASK_CONFIG\n",
    "from wofscast.model import shard_xarray_dataset, replicate_for_devices\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray #as xr\n",
    "\n",
    "from wofscast.utils import count_total_parameters, save_model_params, load_model_params \n",
    "\n",
    "# For training the weights!\n",
    "import optax\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax import device_put\n",
    "from jax import pmap, device_put, local_device_count\n",
    "# Check available devices\n",
    "print(\"Available GPU devices:\", jax.devices())\n",
    "from jax import tree_util\n",
    "\n",
    "import time \n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2059b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial params: mesh = 5, latent\n",
    "mesh_size = 5\n",
    "latent_size = 128\n",
    "gnn_msg_steps = 4\n",
    "hidden_layers = 1\n",
    "grid_to_mesh_node_dist=5\n",
    "\n",
    "task_config = WOFS_TASK_CONFIG\n",
    "\n",
    "model_config = gencast.ModelConfig(\n",
    "      resolution=0,\n",
    "      mesh_size=mesh_size,\n",
    "      latent_size=latent_size,\n",
    "      gnn_msg_steps=gnn_msg_steps,\n",
    "      hidden_layers=hidden_layers,\n",
    "      grid_to_mesh_node_dist=grid_to_mesh_node_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/work/mflora/wofs-cast-data/normalization_stats'\n",
    "\n",
    "mean_by_level = xarray.load_dataset(os.path.join(path, 'mean_by_level.nc'))\n",
    "stddev_by_level = xarray.load_dataset(os.path.join(path, 'stddev_by_level.nc'))\n",
    "diffs_stddev_by_level = xarray.load_dataset(os.path.join(path, 'diffs_stddev_by_level.nc'))\n",
    "\n",
    "norm_stats = {'mean_by_level': mean_by_level, \n",
    "                      'stddev_by_level' : stddev_by_level,\n",
    "                      'diffs_stddev_by_level' : diffs_stddev_by_level\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from os.path import join\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "base_path = '/work/mflora/wofs-cast-data/datasets_zarr'#_zarr'\n",
    "years = ['2019']#, '2020']\n",
    "\n",
    "def get_files_for_year(year):\n",
    "    year_path = join(base_path, year)\n",
    "    with os.scandir(year_path) as it:\n",
    "        return [join(year_path, entry.name) for entry in it if entry.is_dir() and entry.name.endswith('.zarr')]\n",
    "        #return [join(year_path, entry.name) for entry in it if entry.is_file()]\n",
    "    \n",
    "with ThreadPoolExecutor() as executor:\n",
    "    paths = []\n",
    "    for files in executor.map(get_files_for_year, years):\n",
    "        paths.extend(files)\n",
    "\n",
    "print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "generator = ZarrDataGenerator(task_config, \n",
    "                              cpu_batch_size=batch_size, \n",
    "                              gpu_batch_size=batch_size, n_workers=16)\n",
    "\n",
    "j=0\n",
    "for inputs, targets, forcings in generator(paths[:batch_size]):\n",
    "    print(f'Batch : {j}')\n",
    "    j+=1\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_wrapped_graphcast(model_config: gencast.ModelConfig, \n",
    "                                task_config: gencast.TaskConfig,\n",
    "                                norm_stats: dict\n",
    "                               ):\n",
    "    \"\"\"Constructs and wraps the GraphCast Predictor.\"\"\"\n",
    "    # Deeper one-step predictor.\n",
    "    predictor = gencast.GenCast(model_config, task_config)\n",
    "\n",
    "    # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to\n",
    "    # from/to float32 to/from BFloat16.\n",
    "    predictor = casting.Bfloat16Cast(predictor)\n",
    "\n",
    "    # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from\n",
    "    # BFloat16 happens after applying normalization to the inputs/targets.\n",
    "    predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=norm_stats['diffs_stddev_by_level'],\n",
    "      mean_by_level=norm_stats['mean_by_level'],\n",
    "      stddev_by_level=norm_stats['stddev_by_level']\n",
    "    )\n",
    "\n",
    "    # Wraps everything so the one-step model can produce trajectories.\n",
    "    predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "def train_step(params, \n",
    "             state, \n",
    "             opt_state, \n",
    "             optimizer, \n",
    "             inputs, \n",
    "             targets, \n",
    "             forcings, \n",
    "             model_config, \n",
    "             task_config, \n",
    "             norm_stats):\n",
    "    \n",
    "    def compute_loss(params, state, inputs, targets, forcings):\n",
    "        (loss, diagnostics), next_state = loss_fn.apply(params, state, \n",
    "                                                        jax.random.PRNGKey(0), \n",
    "                                                        model_config, \n",
    "                                                        task_config, norm_stats, \n",
    "                                                        inputs, targets, forcings)\n",
    "        return loss, (diagnostics, next_state)\n",
    "    \n",
    "    # Compute gradients and auxiliary outputs\n",
    "    (loss, (diagnostics, next_state)), grads = jax.value_and_grad(compute_loss, has_aux=True)(params, state, \n",
    "                                                                                              inputs, targets, \n",
    "                                                                                              forcings)\n",
    "    \n",
    "    # Combine the gradient across all devices (by taking their mean).\n",
    "    #grads = jax.lax.pmean(grads, axis_name='devices')\n",
    "\n",
    "    # Compute the global norm of all gradients\n",
    "    total_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in tree_util.tree_leaves(grads)))\n",
    "\n",
    "    # Clip gradients if the total norm exceeds the threshold\n",
    "    def clip_grads(g, clip_norm=32):\n",
    "        return jnp.where(total_norm > clip_norm, g * clip_norm / total_norm, g)\n",
    "\n",
    "    clipped_grads = tree_util.tree_map(clip_grads, grads)\n",
    "\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params=params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    return new_params, opt_state\n",
    "\n",
    "# Function for deployment. Used to make predictions on new data and rollout. \n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, norm_stats, inputs, targets_template, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config, norm_stats)\n",
    "    return predictor(inputs, targets_template, forcings)\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, norm_stats, inputs, targets_template, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config, norm_stats)\n",
    "    loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "    return xarray_tree.map_structure(\n",
    "      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "      (loss, diagnostics))\n",
    "\n",
    "def with_configs(fn):\n",
    "    return functools.partial(\n",
    "      fn, model_config=model_config, task_config=task_config, norm_stats=norm_stats)\n",
    "\n",
    "def with_optimizer(fn, optimizer):\n",
    "    return functools.partial(\n",
    "      fn, optimizer=optimizer)\n",
    "\n",
    "# Always pass params and state, so the usage below are simpler\n",
    "def with_model_params(fn):\n",
    "    return functools.partial(fn, params=model_params, state=state)\n",
    "\n",
    "# Our models aren't stateful, so the state is always empty, so just return the\n",
    "# predictions. This is requiredy by our rollout code, and generally simpler.\n",
    "def drop_state(fn):\n",
    "    return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "model_params, state = init_jitted(\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=inputs,\n",
    "    targets_template=targets,\n",
    "    forcings=forcings)\n",
    "\n",
    "run_forward_jitted = drop_state(jax.jit(with_configs(run_forward.apply)))\n",
    "\n",
    "optimizer = optax.adamw(1e-4, b1=0.9, b2=0.95, eps=1e-8, weight_decay=0.1)\n",
    "opt_state = optimizer.init(model_params)\n",
    "\n",
    "train_step_jitted = jax.jit(with_optimizer(with_configs(train_step), optimizer))\n",
    "\n",
    "count = count_total_parameters(model_params)\n",
    "print(f'Number of Model Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f046ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = run_forward_jitted(\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=inputs,\n",
    "    targets_template=targets,\n",
    "    forcings=forcings)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f524b026",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "model_params, opt_state = train_step_jitted(params=model_params, \n",
    "                                            state=state, \n",
    "                                            opt_state=opt_state, \n",
    "                                            inputs=inputs, \n",
    "                                            targets=targets, \n",
    "                                            forcings=forcings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba43f1",
   "metadata": {},
   "source": [
    "# GenCast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(values: xarray.Dataset,\n",
    "              noise: xarray.Dataset,\n",
    "              ) -> xarray.Dataset:\n",
    "  \"\"\"Normalize variables using the given scales and (optionally) locations.\"\"\"\n",
    "  def add_noise_to_array(array):\n",
    "    if array.name is None:\n",
    "      raise ValueError(\n",
    "          \"Can't look up normalization constants because array has no name.\")\n",
    "    if array.name in noise:\n",
    "        array = array + noise[array.name].astype(array.dtype)\n",
    "    else:\n",
    "        logging.warning('No normalization location found for %s', array.name)\n",
    "        \n",
    "    return array\n",
    "\n",
    "  return xarray_tree.map_structure(add_noise_to_array, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import xarray_tree\n",
    "from typing import Mapping\n",
    "\n",
    "def _mean_preserving_batch(x: xarray.DataArray) -> xarray.DataArray:\n",
    "    return x.mean([d for d in x.dims if d != \"batch\"], skipna=False)\n",
    "\n",
    "class EDMLoss:\n",
    "    def __init__(self, P_mean=-1.2,\n",
    "                 P_std=1.2, \n",
    "                 # From GenCast paper, I believe this should = 1.0?\n",
    "                 sigma_data=0.5):\n",
    "        self.P_mean = P_mean\n",
    "        self.P_std = P_std\n",
    "        self.sigma_data = sigma_data\n",
    "\n",
    "    def __call__(self, predictions: xr.Dataset, \n",
    "                 targets: xr.Dataset,\n",
    "                 #per_variable_weights: Mapping[str, float]\n",
    "                ): #-> LossAndDiagnostics:\n",
    "        \n",
    "        def loss(prediction, target):\n",
    "            # Generate random seeds\n",
    "            rnd_normal = jnp.random.randn(*target.shape).astype(target)\n",
    "            sigma = jnp.exp(rnd_normal * self.P_std + self.P_mean)\n",
    "\n",
    "            # Compute weights\n",
    "            weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n",
    "\n",
    "            # Add noise to the targets\n",
    "            n = jnp.random.randn(*target.shape).astype(target) * sigma\n",
    "            noisy_targets = target + n\n",
    "\n",
    "            # Calculate weighted loss\n",
    "            loss = weight * ((prediction - noisy_targets) ** 2)\n",
    "\n",
    "            return _mean_preserving_batch(loss)\n",
    "\n",
    "        losses = xarray_tree.map_structure(loss, predictions, targets)\n",
    "\n",
    "        return losses #sum_per_variable_losses(losses, per_variable_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ac4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edm_loss = EDMLoss() \n",
    "\n",
    "losses = edm_loss(inputs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48385ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def get_random_dataset(dataset, scale):\n",
    "    # Create a copy of the original dataset\n",
    "    new_dataset = dataset.copy()\n",
    "    \n",
    "    # Iterate over all variables in the dataset\n",
    "    for var_name in new_dataset.data_vars:\n",
    "        var = new_dataset[var_name]\n",
    "        # Check if the variable has a 'time' dimension\n",
    "        if 'time' in var.dims:\n",
    "            # Generate random data with the same shape as the variable\n",
    "            random_data = np.random.normal(loc=0.0, scale=scale, size=var.shape)\n",
    "            # Assign the random data to the variable\n",
    "            new_dataset[var_name].data = random_data\n",
    "    \n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "class GenCastSampler:\n",
    "    def __init__(self, gencast, num_steps=18, \n",
    "                 sigma_min=0.03, sigma_max=80, rho=7,\n",
    "                 S_churn=2.5, S_min=0.75, S_max=80, S_noise=1.05):\n",
    "        self.gencast = gencast \n",
    "        self.num_steps = num_steps\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_max = sigma_max\n",
    "        self.rho = rho \n",
    "        self.S_churn = S_churn \n",
    "        self.S_min = S_min \n",
    "        self.S_max = S_max \n",
    "        self.S_noise = S_noise \n",
    "\n",
    "    def inverse_cdf(self, u):\n",
    "        exp = 1/self.rho\n",
    "        return (self.sigma_max**(exp) + u * (self.sigma_min**(exp) - self.sigma_max**(exp)))**self.rho    \n",
    "        \n",
    "    def sample(self, \n",
    "               inputs, \n",
    "               targets_template, \n",
    "               forcings\n",
    "              ):\n",
    "        \n",
    "        # Why is GenCast using a random u? that makes the t_steps random?\n",
    "        \n",
    "        t_steps = [self.inverse_cdf(np.random.uniform(low=0.0, high=1.0, size=1))[0] \n",
    "                   for i in np.arange(self.num_steps) ]\n",
    "        \n",
    "        #step_indices = np.arange(self.num_steps, dtype=jnp.bfloat16)\n",
    "        #t_steps1 = (self.sigma_max ** (1 / self.rho) + step_indices / (self.num_steps - 1))\n",
    "        #t_steps = (t_steps1 * (self.sigma_min ** (1 / self.rho) - self.sigma_max ** (1 / self.rho))) ** self.rho\n",
    "        #t_steps = np.concatenate([np.round(t_steps), np.zeros_like(t_steps[:1])])  # t_N = 0\n",
    "\n",
    "        print(t_steps)\n",
    "        \n",
    "        #x_next = latents.astype(jnp.float64) * t_steps[0]\n",
    "        \n",
    "        # Initialize a xarray.Dataset with random data matching the target dataset?. \n",
    "        x_next = get_random_dataset(targets, scale=t_steps[0])\n",
    "        \n",
    "        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):  # 0, ..., N-1\n",
    "            x_cur = x_next\n",
    "\n",
    "            gamma = min(self.S_churn / self.num_steps, np.sqrt(2) - 1) if self.S_min <= t_cur <= self.S_max else 0\n",
    "            t_hat = np.round(t_cur + gamma * t_cur)\n",
    "            err = get_random_dataset(targets, scale=self.S_noise)\n",
    "            x_hat = x_cur + np.sqrt(t_hat ** 2 - t_cur ** 2) * err\n",
    "\n",
    "            #model_input_images = jnp.concatenate([x_hat, condition_images], axis=1)\n",
    "            denoised = self.gencast(params=model_params, \n",
    "                                    state=state, \n",
    "                                    rng=jax.random.PRNGKey(0),\n",
    "                                    inputs=inputs, targets_template=targets, \n",
    "                                    forcings=forcings, sigma=50.) \n",
    "                    \n",
    "            d_cur = (x_hat - denoised) / t_hat\n",
    "            x_next = x_hat + (t_next - t_hat) * d_cur\n",
    "\n",
    "            if i < self.num_steps - 1:\n",
    "                #model_input_images = jnp.concatenate([x_next, condition_images], axis=1)\n",
    "                #denoised = self.gencast(model_input_images, t_next).astype(jnp.float64)\n",
    "                denoised = self.gencast(params=model_params, \n",
    "                                    state=state, \n",
    "                                    rng=jax.random.PRNGKey(0),\n",
    "                                    inputs=inputs, targets_template=targets, \n",
    "                                    forcings=forcings, sigma=50.) \n",
    "                \n",
    "                d_prime = (x_next - denoised) / t_next\n",
    "                x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "\n",
    "        return x_next\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0415bf37",
   "metadata": {},
   "source": [
    "%%time\n",
    "gencast_predictor = GenCastSampler(run_forward_jitted, num_steps=20)\n",
    "predictions = gencast_predictor.sample(inputs, targets, forcings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e516b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6f66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

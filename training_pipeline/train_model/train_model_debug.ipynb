{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a262f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping gradient checkpointing for sequence length of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "per_variable_weights=None\n",
      "\n",
      "\n",
      "Loss: 19.5703125\n"
     ]
    }
   ],
   "source": [
    "# @title Build jitted functions, and possibly initialize random weights\n",
    "\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "\n",
    "import sys, os \n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "import wofscast.graphcast_lam as graphcast \n",
    "from wofscast.data_generator import (load_wofscast_data, \n",
    "                                    wofscast_data_generator, \n",
    "                                    wofscast_batch_generator, \n",
    "                                    to_static_vars,\n",
    "                                    add_local_solar_time\n",
    "                                    \n",
    "                                    )\n",
    "from wofscast.wofscast_task_config import WOFS_TASK_CONFIG, train_lead_times, TARGET_VARS\n",
    "from wofscast import (data_utils, \n",
    "                      casting, \n",
    "                      normalization,\n",
    "                      autoregressive,\n",
    "                      xarray_tree,\n",
    "                      xarray_jax\n",
    "                     )\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import functools\n",
    "\n",
    "\n",
    "\n",
    "task_config = WOFS_TASK_CONFIG\n",
    "model_config = graphcast.ModelConfig(\n",
    "              resolution=0,\n",
    "              mesh_size=5,\n",
    "              latent_size=16,\n",
    "              gnn_msg_steps=4,\n",
    "              hidden_layers=1,\n",
    "              grid_to_mesh_node_dist=5, \n",
    "              loss_weights = None,\n",
    "              k_hop = 8,\n",
    "              use_transformer = False,\n",
    "              num_attn_heads = 4\n",
    "        )\n",
    "\n",
    "dataset = xr.load_dataset(\n",
    "    '/work/mflora/wofs-cast-data/datasets/2019/wrfwof_2019-05-18_213000_to_2019-05-18_220000__10min__ens_mem_06.nc')\n",
    "\n",
    "dataset = add_local_solar_time(dataset)\n",
    "\n",
    "example_batch = dataset.expand_dims('batch', axis=0)\n",
    "\n",
    "\n",
    "path = '/work/mflora/wofs-cast-data/normalization_stats'\n",
    "\n",
    "mean_by_level = xr.load_dataset(os.path.join(path, 'mean_by_level.nc'))\n",
    "stddev_by_level = xr.load_dataset(os.path.join(path, 'stddev_by_level.nc'))\n",
    "diffs_stddev_by_level = xr.load_dataset(os.path.join(path, 'diffs_stddev_by_level.nc'))\n",
    "\n",
    "train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    example_batch, target_lead_times=\"10min\",\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "train_inputs = to_static_vars(train_inputs)\n",
    "train_inputs = train_inputs.transpose('batch', 'time', 'lat', 'lon', 'level')\n",
    "train_targets = train_targets.transpose('batch', 'time', 'lat', 'lon', 'level')\n",
    "train_forcings = train_forcings.transpose('batch', 'time', 'lat', 'lon')\n",
    "\n",
    "\n",
    "\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config: graphcast.ModelConfig,\n",
    "    task_config: graphcast.TaskConfig):\n",
    "  \"\"\"Constructs and wraps the GraphCast Predictor.\"\"\"\n",
    "  # Deeper one-step predictor.\n",
    "  predictor = graphcast.GraphCast(model_config, task_config)\n",
    "\n",
    "  # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to\n",
    "  # from/to float32 to/from BFloat16.\n",
    "  predictor = casting.Bfloat16Cast(predictor)\n",
    "\n",
    "  # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from\n",
    "  # BFloat16 happens after applying normalization to the inputs/targets.\n",
    "  predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=diffs_stddev_by_level,\n",
    "      mean_by_level=mean_by_level,\n",
    "      stddev_by_level=stddev_by_level)\n",
    "\n",
    "  # Wraps everything so the one-step model can produce trajectories.\n",
    "  predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "  return predictor\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, inputs, targets, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "\n",
    "  return xarray_tree.map_structure(\n",
    "      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "      (loss, diagnostics))\n",
    "\n",
    "def grads_fn(params, state, model_config, task_config, inputs, targets, forcings):\n",
    "  def _aux(params, state, i, t, f):\n",
    "    (loss, diagnostics), next_state = loss_fn.apply(\n",
    "        params, state, jax.random.PRNGKey(0), model_config, task_config,\n",
    "        i, t, f)\n",
    "    return loss, (diagnostics, next_state)\n",
    "\n",
    "  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(\n",
    "      _aux, has_aux=True)(params, state, inputs, targets, forcings)\n",
    "  \n",
    "  # Ensure loss is a concrete value\n",
    "  concrete_loss = jax.device_get(loss)\n",
    "  print(f\"Concrete loss value: {concrete_loss}\") \n",
    "\n",
    "\n",
    "  return loss, diagnostics, next_state, grads\n",
    "\n",
    "# Jax doesn't seem to like passing configs as args through the jit. Passing it\n",
    "# in via partial (instead of capture by closure) forces jax to invalidate the\n",
    "# jit cache if you change configs.\n",
    "def with_configs(fn):\n",
    "  return functools.partial(\n",
    "      fn, model_config=model_config, task_config=task_config)\n",
    "\n",
    "# Always pass params and state, so the usage below are simpler\n",
    "def with_params(fn):\n",
    "  return functools.partial(fn, params=params, state=state)\n",
    "\n",
    "# Our models aren't stateful, so the state is always empty, so just return the\n",
    "# predictions. This is requiredy by our rollout code, and generally simpler.\n",
    "def drop_state(fn):\n",
    "  return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "params = None\n",
    "\n",
    "if params is None:\n",
    "  params, state = init_jitted(\n",
    "      rng=jax.random.PRNGKey(0),\n",
    "      inputs=train_inputs,\n",
    "      targets_template=train_targets,\n",
    "      forcings=train_forcings)\n",
    "\n",
    "loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))\n",
    "grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn)))\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(\n",
    "    run_forward.apply))))\n",
    "\n",
    "\n",
    "\n",
    "# @title Loss computation (autoregressive loss over multiple steps)\n",
    "loss, diagnostics = loss_fn_jitted(\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=train_inputs,\n",
    "    targets=train_targets,\n",
    "    forcings=train_forcings)\n",
    "\n",
    "print(\"Loss:\", float(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d627ee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "per_variable_weights=None\n",
      "\n",
      "\n",
      "Concrete loss value: Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "# modify the gradients function signature\n",
    "def grads_fn(params, state, inputs, targets, forcings, model_config, task_config):\n",
    "    def _aux(params, state, i, t, f):\n",
    "        (loss, diagnostics), next_state = loss_fn.apply(params, state, jax.random.PRNGKey(0), \n",
    "                                                        model_config, task_config, i, t, f)\n",
    "        return loss, (diagnostics, next_state)\n",
    "    (loss, (diagnostics, next_state)), grads = jax.value_and_grad(_aux, has_aux=True)(params, \n",
    "                                                                                      state, inputs, \n",
    "                                                                                      targets, forcings)\n",
    "    \n",
    "    # Ensure loss is a concrete value\n",
    "    concrete_loss = jax.device_get(loss)\n",
    "    print(f\"Concrete loss value: {concrete_loss}\") \n",
    "    \n",
    "    return loss, diagnostics, next_state, grads\n",
    "\n",
    "# remove `with_params` from jitted grads function\n",
    "grads_fn_jitted = jax.jit(with_configs(grads_fn))\n",
    "\n",
    "# setup optimiser\n",
    "lr = 1e-3\n",
    "optimiser = optax.adam(lr, b1=0.9, b2=0.999, eps=1e-8)\n",
    "opt_state = optimiser.init(params)\n",
    "\n",
    "# calculate loss and gradients\n",
    "loss, diagnostics, next_state, grads = grads_fn_jitted(params, state, \n",
    "                                                       train_inputs, train_targets, train_forcings)\n",
    "\n",
    "# update\n",
    "updates, opt_state = optimiser.update(grads, opt_state)\n",
    "params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ea6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fn(params, state, opt_state, inputs, targets, forcings, model_config, task_config, norm_stats):\n",
    "    # Clip gradients if the total norm exceeds the threshold\n",
    "    def clip_grads(g, clip_norm=32):\n",
    "        return jnp.where(total_norm > clip_norm, g * clip_norm / total_norm, g)\n",
    "    \n",
    "    def compute_loss(params, state, inputs, targets, forcings):\n",
    "        print('This step inside compute_loss happened....')\n",
    "        (loss, diagnostics), next_state = loss_fn.apply(params, state, \n",
    "                                                        jax.random.PRNGKey(0), \n",
    "                                                        model_config, \n",
    "                                                        task_config, norm_stats, \n",
    "                                                        inputs, targets, forcings)\n",
    "        return loss, (diagnostics, next_state)\n",
    "    \n",
    "    # Compute gradients and auxiliary outputs\n",
    "    (loss, (diagnostics, next_state)), grads = jax.value_and_grad(compute_loss, has_aux=True)(params, state, \n",
    "                                                                                              inputs, targets, \n",
    "                                                                                              forcings)\n",
    "  \n",
    "    # Combine the gradient across all devices (by taking their mean).\n",
    "    grads = jax.lax.pmean(grads, axis_name='devices')\n",
    "\n",
    "    # Compute the global norm of all gradients and clip them. \n",
    "    total_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in tree_util.tree_leaves(grads)))\n",
    "    clipped_grads = tree_util.tree_map(clip_grads, grads)\n",
    "    \n",
    "    #update \n",
    "    updates, new_opt_state = optimizer.update(clipped_grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return new_params, new_opt_state, loss, diagnostics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

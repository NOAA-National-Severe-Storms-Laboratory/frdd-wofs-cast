{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c770e344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9b8b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU devices: [cuda(id=0), cuda(id=1)]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import sys, os \n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "# Set JAX_TRACEBACK_FILTERING to off for detailed traceback\n",
    "#os.environ['JAX_TRACEBACK_FILTERING'] = 'on'\n",
    "\n",
    "\n",
    "# @title Imports\n",
    "import dataclasses\n",
    "import datetime\n",
    "import functools\n",
    "import math\n",
    "import re\n",
    "from typing import Optional\n",
    "from glob import glob\n",
    "import gc\n",
    "import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "#from google.cloud import storage\n",
    "from wofscast import autoregressive #_lam as autoregressive\n",
    "from wofscast import casting\n",
    "from wofscast import checkpoint\n",
    "from wofscast import data_utils\n",
    "from wofscast import graphcast_lam as graphcast\n",
    "from wofscast import normalization\n",
    "from wofscast import rollout\n",
    "from wofscast import xarray_jax\n",
    "from wofscast import xarray_tree\n",
    "from wofscast.data_generator import (ZarrDataGenerator, \n",
    "                                     add_local_solar_time, \n",
    "                                     to_static_vars, \n",
    "                                     open_zarr,\n",
    "                                     dataset_to_input\n",
    "                                    )\n",
    "\n",
    "from wofscast.wofscast_task_config import DBZ_TASK_CONFIG, WOFS_TASK_CONFIG\n",
    "from wofscast.model import shard_xarray_dataset, replicate_for_devices\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray #as xr\n",
    "\n",
    "from wofscast.utils import count_total_parameters, save_model_params, load_model_params \n",
    "\n",
    "# For training the weights!\n",
    "import optax\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax import device_put\n",
    "from jax import pmap, device_put, local_device_count\n",
    "# Check available devices\n",
    "print(\"Available GPU devices:\", jax.devices())\n",
    "from jax import tree_util\n",
    "\n",
    "import time \n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a98c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 36.7M GraphCast weights. \n",
    "\n",
    "name = 'params_GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output.npz'\n",
    "graphcast_path = os.path.join('/work/mflora/wofs-cast-data/graphcast_models', name)\n",
    "\n",
    "with open(graphcast_path, 'rb') as f:\n",
    "    data = checkpoint.load(graphcast_path, dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3fc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphcast_params = data['params']\n",
    "model_config = data['model_config']\n",
    "#task_config = ckpt.task_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7742cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(params, indent=0):\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(' ' * indent + f\"{key}:\")\n",
    "            print_params(value, indent + 2)\n",
    "        else:\n",
    "            print(' ' * indent + f\"{key}: {value.shape}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2059b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial params: mesh = 5, latent\n",
    "mesh_size = 5\n",
    "latent_size = 512\n",
    "gnn_msg_steps = 16\n",
    "hidden_layers = 1\n",
    "grid_to_mesh_node_dist=5\n",
    "\n",
    "task_config = WOFS_TASK_CONFIG\n",
    "\n",
    "model_config = graphcast.ModelConfig(\n",
    "      resolution=0,\n",
    "      mesh_size=mesh_size,\n",
    "      latent_size=latent_size,\n",
    "      gnn_msg_steps=gnn_msg_steps,\n",
    "      hidden_layers=hidden_layers,\n",
    "      grid_to_mesh_node_dist=grid_to_mesh_node_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb7f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/work/mflora/wofs-cast-data/full_normalization_stats'\n",
    "\n",
    "mean_by_level = xarray.load_dataset(os.path.join(path, 'mean_by_level.nc'))\n",
    "stddev_by_level = xarray.load_dataset(os.path.join(path, 'stddev_by_level.nc'))\n",
    "diffs_stddev_by_level = xarray.load_dataset(os.path.join(path, 'diffs_stddev_by_level.nc'))\n",
    "\n",
    "norm_stats = {'mean_by_level': mean_by_level, \n",
    "                      'stddev_by_level' : stddev_by_level,\n",
    "                      'diffs_stddev_by_level' : diffs_stddev_by_level\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6a39cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8730\n",
      "CPU times: user 4.05 ms, sys: 7.25 ms, total: 11.3 ms\n",
      "Wall time: 10.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "from os.path import join\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "base_path = '/work/mflora/wofs-cast-data/datasets_zarr'#_zarr'\n",
    "years = ['2019']#, '2020']\n",
    "\n",
    "def get_files_for_year(year):\n",
    "    year_path = join(base_path, year)\n",
    "    with os.scandir(year_path) as it:\n",
    "        return [join(year_path, entry.name) for entry in it if entry.is_dir() and entry.name.endswith('.zarr')]\n",
    "        #return [join(year_path, entry.name) for entry in it if entry.is_file()]\n",
    "    \n",
    "with ThreadPoolExecutor() as executor:\n",
    "    paths = []\n",
    "    for files in executor.map(get_files_for_year, years):\n",
    "        paths.extend(files)\n",
    "\n",
    "print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be10df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mflora/miniconda3/envs/wofs-cast/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/work/mflora/miniconda3/envs/wofs-cast/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch : 0\n",
      "CPU times: user 208 ms, sys: 336 ms, total: 544 ms\n",
      "Wall time: 636 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "generator = ZarrDataGenerator(task_config, \n",
    "                              cpu_batch_size=batch_size, \n",
    "                              gpu_batch_size=batch_size, n_workers=16)\n",
    "\n",
    "j=0\n",
    "for inputs, targets, forcings in generator(paths[:batch_size]):\n",
    "    print(f'Batch : {j}')\n",
    "    j+=1\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a6c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  7 16:23:47 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:21:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             67W /  300W |   62134MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             68W /  300W |    1268MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    714984      C   ...iniconda3/envs/wofs-cast/bin/python      61238MiB |\n",
      "|    0   N/A  N/A    932379      C   ...iniconda3/envs/wofs-cast/bin/python        444MiB |\n",
      "|    0   N/A  N/A   1606530      C   ...iniconda3/envs/wofs-cast/bin/python        432MiB |\n",
      "|    1   N/A  N/A    714984      C   ...iniconda3/envs/wofs-cast/bin/python        416MiB |\n",
      "|    1   N/A  N/A    932379      C   ...iniconda3/envs/wofs-cast/bin/python        416MiB |\n",
      "|    1   N/A  N/A   1606530      C   ...iniconda3/envs/wofs-cast/bin/python        416MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    print(sp.check_output(\"nvidia-smi\").decode('ascii'))\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941d2d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping gradient checkpointing for sequence length of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Model Parameters: 36015209\n"
     ]
    }
   ],
   "source": [
    "def construct_wrapped_graphcast(model_config: graphcast.ModelConfig, \n",
    "                                task_config: graphcast.TaskConfig,\n",
    "                                norm_stats: dict\n",
    "                               ):\n",
    "    \"\"\"Constructs and wraps the GraphCast Predictor.\"\"\"\n",
    "    # Deeper one-step predictor.\n",
    "    predictor = graphcast.GraphCast(model_config, task_config)\n",
    "\n",
    "    # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to\n",
    "    # from/to float32 to/from BFloat16.\n",
    "    predictor = casting.Bfloat16Cast(predictor)\n",
    "\n",
    "    # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from\n",
    "    # BFloat16 happens after applying normalization to the inputs/targets.\n",
    "    predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=norm_stats['diffs_stddev_by_level'],\n",
    "      mean_by_level=norm_stats['mean_by_level'],\n",
    "      stddev_by_level=norm_stats['stddev_by_level']\n",
    "    )\n",
    "\n",
    "    # Wraps everything so the one-step model can produce trajectories.\n",
    "    predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "def train_step(params, \n",
    "             state, \n",
    "             opt_state, \n",
    "             optimizer, \n",
    "             inputs, \n",
    "             targets, \n",
    "             forcings, \n",
    "             model_config, \n",
    "             task_config, \n",
    "             norm_stats):\n",
    "    \n",
    "    def compute_loss(params, state, inputs, targets, forcings):\n",
    "        (loss, diagnostics), next_state = loss_fn.apply(params, state, \n",
    "                                                        jax.random.PRNGKey(0), \n",
    "                                                        model_config, \n",
    "                                                        task_config, norm_stats, \n",
    "                                                        inputs, targets, forcings)\n",
    "        return loss, (diagnostics, next_state)\n",
    "    \n",
    "    # Compute gradients and auxiliary outputs\n",
    "    (loss, (diagnostics, next_state)), grads = jax.value_and_grad(compute_loss, has_aux=True)(params, state, \n",
    "                                                                                              inputs, targets, \n",
    "                                                                                              forcings)\n",
    "    \n",
    "    # Combine the gradient across all devices (by taking their mean).\n",
    "    #grads = jax.lax.pmean(grads, axis_name='devices')\n",
    "\n",
    "    # Compute the global norm of all gradients\n",
    "    total_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in tree_util.tree_leaves(grads)))\n",
    "\n",
    "    # Clip gradients if the total norm exceeds the threshold\n",
    "    def clip_grads(g, clip_norm=32):\n",
    "        return jnp.where(total_norm > clip_norm, g * clip_norm / total_norm, g)\n",
    "\n",
    "    clipped_grads = tree_util.tree_map(clip_grads, grads)\n",
    "\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params=params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    return new_params, opt_state\n",
    "\n",
    "# Function for deployment. Used to make predictions on new data and rollout. \n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, norm_stats, inputs, targets_template, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config, norm_stats)\n",
    "    return predictor(inputs, targets_template, forcings)\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, norm_stats, inputs, targets_template, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config, norm_stats)\n",
    "    loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "    return xarray_tree.map_structure(\n",
    "      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "      (loss, diagnostics))\n",
    "\n",
    "def with_configs(fn):\n",
    "    return functools.partial(\n",
    "      fn, model_config=model_config, task_config=task_config, norm_stats=norm_stats)\n",
    "\n",
    "def with_optimizer(fn, optimizer):\n",
    "    return functools.partial(\n",
    "      fn, optimizer=optimizer)\n",
    "\n",
    "# Always pass params and state, so the usage below are simpler\n",
    "def with_model_params(fn):\n",
    "    return functools.partial(fn, params=model_params, state=state)\n",
    "\n",
    "# Our models aren't stateful, so the state is always empty, so just return the\n",
    "# predictions. This is requiredy by our rollout code, and generally simpler.\n",
    "def drop_state(fn):\n",
    "    return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "model_params, state = init_jitted(\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=inputs,\n",
    "    targets_template=targets,\n",
    "    forcings=forcings)\n",
    "\n",
    "#run_forward_jitted = drop_state(jax.jit(with_configs(run_forward.apply)))\n",
    "\n",
    "optimizer = optax.adamw(1e-4, b1=0.9, b2=0.95, eps=1e-8, weight_decay=0.1)\n",
    "opt_state = optimizer.init(model_params)\n",
    "\n",
    "train_step_jitted = jax.jit(with_optimizer(with_configs(train_step), optimizer))\n",
    "\n",
    "count = count_total_parameters(model_params)\n",
    "print(f'Number of Model Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189809d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params_with_graphcast(model_params, graphcast_params):\n",
    "    def update_recursive(model_dict, graphcast_dict):\n",
    "        for key, value in model_dict.items():\n",
    "            if isinstance(value, dict) and key in graphcast_dict and isinstance(graphcast_dict[key], dict):\n",
    "                # If both are dictionaries, recurse\n",
    "                update_recursive(model_dict[key], graphcast_dict[key])\n",
    "            elif key in graphcast_dict and model_dict[key].shape == graphcast_dict[key].shape:\n",
    "                # If both shapes are identical, update the value\n",
    "                model_dict[key] = graphcast_dict[key]\n",
    "    \n",
    "    update_recursive(model_params, graphcast_params)\n",
    "    return model_params\n",
    "\n",
    "# Example usage:\n",
    "updated_model_params = update_params_with_graphcast(model_params, graphcast_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93397097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  7 16:23:54 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:21:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             69W /  300W |   62642MiB /  81920MiB |      1%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             68W /  300W |    1268MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    714984      C   ...iniconda3/envs/wofs-cast/bin/python      61238MiB |\n",
      "|    0   N/A  N/A    932379      C   ...iniconda3/envs/wofs-cast/bin/python        444MiB |\n",
      "|    0   N/A  N/A   1606530      C   ...iniconda3/envs/wofs-cast/bin/python        940MiB |\n",
      "|    1   N/A  N/A    714984      C   ...iniconda3/envs/wofs-cast/bin/python        416MiB |\n",
      "|    1   N/A  N/A    932379      C   ...iniconda3/envs/wofs-cast/bin/python        416MiB |\n",
      "|    1   N/A  N/A   1606530      C   ...iniconda3/envs/wofs-cast/bin/python        416MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66946487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc228533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 2  |  3.602e+07  |  512  | 16    |   1  \n"
     ]
    }
   ],
   "source": [
    "print(f'| {batch_size}  |  {count:.3e}  |  {latent_size}  | {gnn_msg_steps}    |   {hidden_layers}  ')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c670de81",
   "metadata": {},
   "source": [
    "# GPU Usage for WOFS_TASK_CONFIG\n",
    "\n",
    "# batch size  |  model parameters  | Latent Size | GNN MSG STEPS| Hidden Layers |  GPU mem | Time\n",
    "| 16          |  1.420e+06         |  128        | 8            |   1           |   25 GB\n",
    "| 32          |  1.420e+06         |  128        | 8            |   1           |   50 GB \n",
    "| 8           |  3.612e+07         |  512        | 16           |   1           |   50 GB\n",
    "| 1           |  1.435e+08         |  1024       | 16           |   1           |   13 GB\n",
    "| 1           |  1.897e+08         |  1024       | 16           |   2           |   25 GB \n",
    "| 1           |  5.723e+08         |  2048       | 16           |   1           |   33 GB\n",
    "| 1           |  7.569e+08         |  2048       | 16           |   2           |   61 GB\n",
    "\n",
    "| 8           |  3.602e+07         |  512        | 16           |   1           |  27 s\n",
    "\n",
    "| 8  |  2.130e+07  |  512  | 8    |   1  25.9 grid_to_mesh_node_dist = 5\n",
    "\n",
    "13.2s for 8, 17s for 16"
   ]
  },
  {
   "cell_type": "raw",
   "id": "347bc3ba",
   "metadata": {},
   "source": [
    "# GPU Usage for DBZ_TASK_CONFIG\n",
    "\n",
    "# batch size  |  model parameters  | Latent Size | GNN MSG STEPS| Hidden Layers |  GPU mem | Time\n",
    "| 16          |  3.575e+07         |  512        | 16           |   1           |   OOM    |\n",
    "| 8           |  3.575e+07         |  512        | 16           |   1           |   50 GB  | 25.6 s\n",
    "\n",
    "\n",
    "11.8 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be82c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  7 16:24:15 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:21:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             67W /  300W |   74478MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             67W /  300W |    1268MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    714984      C   ...iniconda3/envs/wofs-cast/bin/python      61238MiB |\n",
      "|    0   N/A  N/A    932379      C   ...iniconda3/envs/wofs-cast/bin/python        444MiB |\n",
      "|    0   N/A  N/A   1606530      C   ...iniconda3/envs/wofs-cast/bin/python      12776MiB |\n",
      "|    1   N/A  N/A    714984      C   ...iniconda3/envs/wofs-cast/bin/python        416MiB |\n",
      "|    1   N/A  N/A    932379      C   ...iniconda3/envs/wofs-cast/bin/python        416MiB |\n",
      "|    1   N/A  N/A   1606530      C   ...iniconda3/envs/wofs-cast/bin/python        416MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "CPU times: user 39.6 s, sys: 17 s, total: 56.6 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_params, opt_state = train_step_jitted(params=updated_model_params, \n",
    "                                            state=state, \n",
    "                                            opt_state=opt_state, \n",
    "                                            inputs=inputs, \n",
    "                                            targets=targets, \n",
    "                                            forcings=forcings)\n",
    "get_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c80b2afc",
   "metadata": {},
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def calculate_model_params_size(model_params):\n",
    "    def _get_nbytes(x):\n",
    "        if isinstance(x, jax.Array):\n",
    "            return x.nbytes\n",
    "        elif isinstance(x, (list, tuple)):\n",
    "            return sum(_get_nbytes(item) for item in x)\n",
    "        elif isinstance(x, dict):\n",
    "            return sum(_get_nbytes(value) for value in x.values())\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    total_bytes = _get_nbytes(model_params)\n",
    "    total_gb = total_bytes / (1024 ** 2)\n",
    "    return total_gb\n",
    "\n",
    "# Example usage\n",
    "model_params_size_gb = calculate_model_params_size(model_params)\n",
    "print(f\"Model parameters size: {model_params_size_gb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c87ca63",
   "metadata": {},
   "source": [
    "# Calculate the memory size in bytes\n",
    "inputs_size_bytes = inputs.nbytes\n",
    "targets_size_bytes = targets.nbytes\n",
    "forcings_size_bytes = forcings.nbytes\n",
    "\n",
    "# Convert bytes to gigabytes\n",
    "bytes_in_gb = 1024 ** 3\n",
    "inputs_size_gb = inputs_size_bytes / bytes_in_gb\n",
    "targets_size_gb = targets_size_bytes / bytes_in_gb\n",
    "forcings_size_gb = forcings_size_bytes / bytes_in_gb\n",
    "\n",
    "print(f\"Inputs size: {inputs_size_gb:.2f} GB\")\n",
    "print(f\"Targets size: {targets_size_gb:.2f} GB\")\n",
    "print(f\"Forcings size: {forcings_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba43f1",
   "metadata": {},
   "source": [
    "# GenCast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b9bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(values: xarray.Dataset,\n",
    "              noise: xarray.Dataset,\n",
    "              ) -> xarray.Dataset:\n",
    "  \"\"\"Normalize variables using the given scales and (optionally) locations.\"\"\"\n",
    "  def add_noise_to_array(array):\n",
    "    if array.name is None:\n",
    "      raise ValueError(\n",
    "          \"Can't look up normalization constants because array has no name.\")\n",
    "    if array.name in noise:\n",
    "        array = array + noise[array.name].astype(array.dtype)\n",
    "    else:\n",
    "        logging.warning('No normalization location found for %s', array.name)\n",
    "        \n",
    "    return array\n",
    "\n",
    "  return xarray_tree.map_structure(add_noise_to_array, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48385ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def get_random_dataset(dataset, scale):\n",
    "    # Create a copy of the original dataset\n",
    "    new_dataset = dataset.copy()\n",
    "    \n",
    "    # Iterate over all variables in the dataset\n",
    "    for var_name in new_dataset.data_vars:\n",
    "        var = new_dataset[var_name]\n",
    "        # Check if the variable has a 'time' dimension\n",
    "        if 'time' in var.dims:\n",
    "            # Generate random data with the same shape as the variable\n",
    "            random_data = np.random.normal(loc=0.0, scale=scale, size=var.shape)\n",
    "            # Assign the random data to the variable\n",
    "            new_dataset[var_name].data = random_data\n",
    "    \n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "\n",
    "class GenCastSampler:\n",
    "    def __init__(self, gencast, num_steps=18, \n",
    "                 sigma_min=0.03, sigma_max=80, rho=7,\n",
    "                 S_churn=2.5, S_min=0.75, S_max=80, S_noise=1.05):\n",
    "        self.gencast = gencast \n",
    "        self.num_steps = num_steps\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_max = sigma_max\n",
    "        self.rho = rho \n",
    "        self.S_churn = S_churn \n",
    "        self.S_min = S_min \n",
    "        self.S_max = S_max \n",
    "        self.S_noise = S_noise \n",
    "\n",
    "    def inverse_cdf(self, u):\n",
    "        exp = 1/self.rho\n",
    "        return (self.sigma_max**(exp) + u * (self.sigma_min**(exp) - self.sigma_max**(exp)))**self.rho    \n",
    "        \n",
    "    def sample(self, \n",
    "               inputs, \n",
    "               targets_template, \n",
    "               forcings\n",
    "              ):\n",
    "        \n",
    "        # Why is GenCast using a random u? that makes the t_steps random?\n",
    "        \n",
    "        t_steps = [self.inverse_cdf(np.random.uniform(low=0.0, high=1.0, size=1))[0] \n",
    "                   for i in np.arange(self.num_steps) ]\n",
    "        \n",
    "        #step_indices = np.arange(self.num_steps, dtype=jnp.bfloat16)\n",
    "        #t_steps1 = (self.sigma_max ** (1 / self.rho) + step_indices / (self.num_steps - 1))\n",
    "        #t_steps = (t_steps1 * (self.sigma_min ** (1 / self.rho) - self.sigma_max ** (1 / self.rho))) ** self.rho\n",
    "        #t_steps = np.concatenate([np.round(t_steps), np.zeros_like(t_steps[:1])])  # t_N = 0\n",
    "\n",
    "        print(t_steps)\n",
    "        \n",
    "        #x_next = latents.astype(jnp.float64) * t_steps[0]\n",
    "        \n",
    "        # Initialize a xarray.Dataset with random data matching the target dataset?. \n",
    "        x_next = get_random_dataset(targets, scale=t_steps[0])\n",
    "        \n",
    "        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):  # 0, ..., N-1\n",
    "            x_cur = x_next\n",
    "\n",
    "            gamma = min(self.S_churn / self.num_steps, np.sqrt(2) - 1) if self.S_min <= t_cur <= self.S_max else 0\n",
    "            t_hat = np.round(t_cur + gamma * t_cur)\n",
    "            err = get_random_dataset(targets, scale=self.S_noise)\n",
    "            x_hat = x_cur + np.sqrt(t_hat ** 2 - t_cur ** 2) * err\n",
    "\n",
    "            #model_input_images = jnp.concatenate([x_hat, condition_images], axis=1)\n",
    "            denoised = self.gencast(params=model_params, \n",
    "                                    state=state, \n",
    "                                    rng=jax.random.PRNGKey(0),\n",
    "                                    inputs=inputs, targets_template=targets, \n",
    "                                    forcings=forcings, sigma=50.) \n",
    "                    \n",
    "            d_cur = (x_hat - denoised) / t_hat\n",
    "            x_next = x_hat + (t_next - t_hat) * d_cur\n",
    "\n",
    "            if i < self.num_steps - 1:\n",
    "                #model_input_images = jnp.concatenate([x_next, condition_images], axis=1)\n",
    "                #denoised = self.gencast(model_input_images, t_next).astype(jnp.float64)\n",
    "                denoised = self.gencast(params=model_params, \n",
    "                                    state=state, \n",
    "                                    rng=jax.random.PRNGKey(0),\n",
    "                                    inputs=inputs, targets_template=targets, \n",
    "                                    forcings=forcings, sigma=50.) \n",
    "                \n",
    "                d_prime = (x_next - denoised) / t_next\n",
    "                x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "\n",
    "        return x_next\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0415bf37",
   "metadata": {},
   "source": [
    "%%time\n",
    "gencast_predictor = GenCastSampler(run_forward_jitted, num_steps=20)\n",
    "predictions = gencast_predictor.sample(inputs, targets, forcings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44e516b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6f66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

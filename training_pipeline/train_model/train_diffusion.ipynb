{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from wofscast.model import WoFSCastModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd54d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/work/mflora/wofs-cast-data/model/wofscast_baseline_full_v2.npz'\n",
    "model = WoFSCastModel()\n",
    "model.load_model(MODEL_PATH)\n",
    "\n",
    "path = os.path.join(base_path, name)\n",
    "dataset = xarray.load_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ee537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_wrapped_graphcast(model_config: graphcast.ModelConfig, \n",
    "                                task_config: graphcast.TaskConfig,\n",
    "                                norm_stats: dict\n",
    "                               ):\n",
    "    \"\"\"Constructs and wraps the GraphCast Predictor.\"\"\"\n",
    "    # Deeper one-step predictor.\n",
    "    predictor = graphcast.GraphCast(model_config, task_config)\n",
    "\n",
    "    # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to\n",
    "    # from/to float32 to/from BFloat16.\n",
    "    predictor = casting.Bfloat16Cast(predictor)\n",
    "\n",
    "    # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from\n",
    "    # BFloat16 happens after applying normalization to the inputs/targets.\n",
    "    predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=norm_stats['diffs_stddev_by_level'],\n",
    "      mean_by_level=norm_stats['mean_by_level'],\n",
    "      stddev_by_level=norm_stats['stddev_by_level']\n",
    "    )\n",
    "\n",
    "    # Wraps everything so the one-step model can produce trajectories.\n",
    "    predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, norm_stats, inputs, targets_template, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config, norm_stats)\n",
    "    return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, norm_stats, inputs, targets, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config, norm_stats)\n",
    "    loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "    return xarray_tree.map_structure(\n",
    "      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "      (loss, diagnostics))\n",
    "\n",
    "# Jax doesn't seem to like passing configs as args through the jit. Passing it\n",
    "# in via partial (instead of capture by closure) forces jax to invalidate the\n",
    "# jit cache if you change configs.\n",
    "def with_configs(fn, model_obj, norm_stats):\n",
    "    return functools.partial(\n",
    "      fn, model_config=model_obj.model_config, task_config=model_obj.task_config, norm_stats=norm_stats)\n",
    "\n",
    "def grads_fn_parallel(params, state, inputs, targets, forcings, model_config, task_config, norm_stats):\n",
    "    def compute_loss(params, state, inputs, targets, forcings):\n",
    "        (loss, diagnostics), next_state = loss_fn.apply(params, state, \n",
    "                                                        jax.random.PRNGKey(0), \n",
    "                                                        model_config, \n",
    "                                                        task_config, norm_stats, \n",
    "                                                        inputs, targets, forcings)\n",
    "        return loss, (diagnostics, next_state)\n",
    "    \n",
    "    # Compute gradients and auxiliary outputs\n",
    "    (loss, (diagnostics, next_state)), grads = jax.value_and_grad(compute_loss, has_aux=True)(params, state, \n",
    "                                                                                              inputs, targets, \n",
    "                                                                                              forcings)\n",
    "    \n",
    "    # Combine the gradient across all devices (by taking their mean).\n",
    "    grads = jax.lax.pmean(grads, axis_name='devices')\n",
    "\n",
    "    # Compute the global norm of all gradients\n",
    "    total_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in tree_util.tree_leaves(grads)))\n",
    "\n",
    "    # Clip gradients if the total norm exceeds the threshold\n",
    "    def clip_grads(g, clip_norm=32):\n",
    "        return jnp.where(total_norm > clip_norm, g * clip_norm / total_norm, g)\n",
    "\n",
    "    clipped_grads = tree_util.tree_map(clip_grads, grads)\n",
    "\n",
    "    return loss, diagnostics, next_state, clipped_grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035260b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion():\n",
    "    def __init__(self): \n",
    "        \n",
    "        \n",
    "    def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

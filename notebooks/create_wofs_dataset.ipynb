{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45ee118",
   "metadata": {},
   "source": [
    "## Data Generation for WoFSCast\n",
    "\n",
    "1. Limit varialbes to wind components, temp, pressure, comp. refl, and 2-5 UH \n",
    "2. Using WRFOUT files from May 2020 \n",
    "3. Only using the center 150 x 150 of a WoFS domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269e925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Files: 204\n",
      "Found 73 files for processing.\n",
      "Saving the training datasets...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'    \\n    \\n#process_dataset(dates[0], init_times[0], 1, drop_vars)    \\nwith ProcessPoolExecutor(max_workers=max_workers) as executor:\\n    futures = [executor.submit(process_dataset, date, init_time, \\n                               mem, drop_vars) for date in dates for mem in mems for init_time in init_times]\\n    for future in tqdm(futures, desc=\"Processing datasets\", total=total_files):\\n        future.result()  # Wait for all futures to complete\\n\\nprint(\\'Done!\\')\\n\\nfrom wofscast import data_utils\\nfrom wofscast import my_graphcast as graphcast\\n\\n# Create the training and target datasets. \\ninput_variables = [\\'U\\', \\'V\\', \\'W\\', \\'T\\']#, \\'P\\', \\'REFL_10CM\\', \\'UP_HELI_MAX\\']\\ntarget_variables = [\\'U\\', \\'V\\', \\'W\\', \\'T\\']#, \\'P\\', \\'REFL_10CM\\', \\'UP_HELI_MAX\\']\\nforcing_variables = [\"XLAND\"]\\n# Not pressure levels, but just vertical array indices at the moment. \\npressure_levels = np.arange(0, 40) #list(np.arange(0,40,2))\\n# Loads data from the past 20 minutes (2 steps) and \\n# creates a target over the next 10-60 min. \\ninput_duration = \\'20min\\'\\ntrain_lead_times = slice(\\'10min\\', \\'60min\\') \\n\\n\\ntask_config = graphcast.TaskConfig(\\n      input_variables=input_variables,\\n      target_variables=target_variables,\\n      forcing_variables=forcing_variables,\\n      pressure_levels=pressure_levels,\\n      input_duration=input_duration,\\n  )\\n\\n# Example usage:\\nchunk_size = 3  # The size of each chunk\\noverlap = 2     # The overlap between consecutive chunks\\n\\ndata_paths = glob(os.path.join(\\'/work/mflora/wofs-cast-data/datasets/dataset*.nc\\'))\\ndata_paths.sort()\\n\\ntrain_input_list = []\\ntrain_target_list = []\\ntrain_forcing_list = []\\n\\ndatasets = []\\n\\nfor path in data_paths:\\n    dataset = xr.load_dataset(path)\\n    \\n    # @title Extract training and eval data\\n    example_batch = dataset.expand_dims(dim=\\'batch\\', axis=0)\\n\\n    _train_inputs, _train_targets, _train_forcings = data_utils.extract_inputs_targets_forcings(\\n            example_batch, target_lead_times=train_lead_times,\\n            **dataclasses.asdict(task_config))\\n\\n    train_input_list.append(_train_inputs)\\n    train_target_list.append(_train_targets)\\n    train_forcing_list.append(_train_forcings)\\n    \\ntrain_inputs = xr.concat(train_input_list, dim=\\'batch\\')\\ntrain_targets = xr.concat(train_target_list, dim=\\'batch\\')\\ntrain_forcings = xr.concat(train_forcing_list, dim=\\'batch\\')\\n    \\nprint(\"All Examples:  \", example_batch.dims.mapping)\\nprint(\"*\"*80)\\nprint(\"Train Inputs:  \", train_inputs.dims.mapping)\\nprint(\"Train Targets: \", train_targets.dims.mapping)\\nprint(\"Train Forcings:\", train_forcings.dims.mapping)\\n\\nbase_path = \\'/work/mflora/wofs-cast-data/train_datasets\\'\\n# Save to NetCDF files\\nprint(\\'Saving the training datasets...\\')\\ntrain_inputs.to_netcdf(os.path.join(base_path, \\'train_inputs.nc\\'))\\ntrain_targets.to_netcdf(os.path.join(base_path, \\'train_targets.nc\\'))\\ntrain_forcings.to_netcdf(os.path.join(base_path, \\'train_forcings.nc\\'))\\n\\nprint(\\'Computing and saving the normalization datasets...\\')\\nfull_dataset = xr.concat([train_inputs, train_forcings], dim=\\'batch\\')\\n\\n# Compute the global mean and standard deviation by level\\nmean_by_level = full_dataset.mean(dim=[\\'time\\', \\'lat\\', \\'lon\\', \\'batch\\'])\\nstddev_by_level = full_dataset.std(dim=[\\'time\\', \\'lat\\', \\'lon\\', \\'batch\\'])\\n\\n# For differences, first compute differences by time within each dataset, then concatenate\\ntime_diffs = full_dataset.diff(dim=\\'time\\')\\n\\n# Compute the global standard deviation of the differences by level\\ndiffs_stddev_by_level = time_diffs.std(dim=[ \\'time\\', \\'lat\\', \\'lon\\', \\'batch\\'])\\n\\n# Save to NetCDF files\\nbase_path = \\'/work/mflora/wofs-cast-data/normalization_stats/\\'\\n\\nmean_by_level.to_netcdf(os.path.join(base_path, \\'mean_by_level.nc\\'))\\nstddev_by_level.to_netcdf(os.path.join(base_path, \\'stddev_by_level.nc\\'))\\ndiffs_stddev_by_level.to_netcdf(os.path.join(base_path, \\'diffs_stddev_by_level.nc\\'))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Generation for WoFSCast \n",
    "#\n",
    "# Using the raw WoFS WRFOUTS stored locally on the NSSL machines, \n",
    "# build a 3D dataset formatted for the Google's GraphCast codebase\n",
    "#\n",
    "# Using the training inputs, compute the normalization statistics. \n",
    "# \n",
    "\n",
    "import sys, os \n",
    "sys.path.append('/home/monte.flora/python_packages/frdd-wofs-cast/')\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "VARS_3D_TO_KEEP = ['U', 'V', 'W', 'T', 'PH', 'PHB']\n",
    "VARS_2D_TO_KEEP = ['T2', 'RAINNC', 'COMPOSITE_REFL_10CM', 'UP_HELI_MAX']\n",
    "CONSTANTS = ['HGT', 'XLAND']\n",
    "\n",
    "VARS_TO_KEEP = VARS_3D_TO_KEEP + VARS_2D_TO_KEEP + CONSTANTS\n",
    "BASE_WRFOUT_PATH = '/work2/wof/realtime/FCST/2020/'\n",
    "\n",
    "\n",
    "# Main parallel processing script\n",
    "init_times = ['2000', '2100', '2200', '2300', '0000', '0100']\n",
    "mems = range(1, 18)\n",
    "dates = os.listdir(BASE_WRFOUT_PATH)\n",
    "dates.sort()\n",
    "dates = dates[5:25]\n",
    "\n",
    "# For Debugging! \n",
    "init_times = ['2000', '2100']\n",
    "dates = dates[:6]\n",
    " \n",
    "total_files = len(dates)*len(mems)*len(init_times)\n",
    "\n",
    "max_workers = min(50, total_files)                \n",
    "                  \n",
    "print(f\"Num of Files: {total_files}\")\n",
    "\n",
    "# Assuming a single latitude longitude grid for all WoFS cases!!\n",
    "this_path = glob(os.path.join(BASE_WRFOUT_PATH, dates[0], '2300', 'ENS_MEM_01', 'wrfwof_d01_*'))[0]\n",
    "with xr.open_dataset(this_path) as this_ds:\n",
    "    data_vars = this_ds.data_vars\n",
    "    drop_vars = [v for v in data_vars if v not in VARS_TO_KEEP]\n",
    "    this_ds = this_ds.compute()\n",
    "    \n",
    "    # Renaming coordinate variables to align with the ERA5 naming convention.\n",
    "    this_ds = this_ds.rename({ \n",
    "                    'XLAT': 'latitude', 'XLONG' : 'longitude', \n",
    "                    'south_north' : 'lat', 'west_east' : 'lon'\n",
    "               })\n",
    "    \n",
    "    # Latitude and longitude are expected to be 1d vectors. \n",
    "    lat_1d = this_ds['latitude'].isel(lon=0, Time=0)\n",
    "    lon_1d = this_ds['longitude'].isel(lat=0, Time=0)\n",
    "\n",
    "\n",
    "# Function to process each dataset\n",
    "def process_dataset(date, init_time, mem, drop_vars):\n",
    "    BASE_WRFOUT_PATH = '/work2/wof/realtime/FCST/2020/'\n",
    "    \n",
    "    try:\n",
    "        fname = f'/work/mflora/wofs-cast-data/eval_datasets/dataset_{date}{init_time}_mem_{mem:02d}.nc'\n",
    "        \n",
    "        #if os.path.exists(fname):\n",
    "        #    return None\n",
    "        \n",
    "        # Start with 10-minute resolution \n",
    "        time_resolution = '10min'\n",
    "    \n",
    "        data_paths = glob(os.path.join(BASE_WRFOUT_PATH, date, init_time,\n",
    "                                       f'ENS_MEM_{mem:02d}', 'wrfwof_d01_*'))\n",
    "        print(f\"Found {len(data_paths)} files for processing.\")\n",
    "        if not data_paths:\n",
    "            print(\"No files found, skipping.\")\n",
    "            return None  # or handle the situation as needed\n",
    "        \n",
    "        data_paths.sort()\n",
    "\n",
    "        # Extract an hour out since WoFS is more accurate \n",
    "        # at that point. Keep an 1 hr worth of forecasts. \n",
    "        data_paths = data_paths[12:68+2:2]\n",
    "        \n",
    "        drop_vars += ['XLAT', 'XLONG']\n",
    "        \n",
    "        ds = xr.open_mfdataset(data_paths, combine='nested', concat_dim='Time', \n",
    "                       drop_variables=drop_vars, engine='netcdf4')   \n",
    "\n",
    "        # Loads the dataset into memory! \n",
    "        ds = ds.compute()\n",
    "\n",
    "        # Combine geopotential perturbation + base state\n",
    "        ds['GEOPOT'] = ds['PH'] + ds['PHB']\n",
    "        ds = ds.drop_vars(['PH', 'PHB'])\n",
    "        \n",
    "        # Renaming coordinate variables to align with the ERA5 naming convention.\n",
    "        ds = ds.rename({'Time': 'time', 'bottom_top' :'level', \n",
    "                    #'XLAT': 'latitude', 'XLONG' : 'longitude', \n",
    "                    'south_north' : 'lat', 'west_east' : 'lon'\n",
    "               })\n",
    "\n",
    "        # Destagger the wind fields \n",
    "        u_destaggered = 0.5 * (ds['U'] + ds['U'].roll(west_east_stag=-1, roll_coords=False))\n",
    "        v_destaggered = 0.5 * (ds['V'] + ds['V'].roll(south_north_stag=-1, roll_coords=False))\n",
    "        w_destaggered = 0.5 * (ds['W'] + ds['W'].roll(bottom_top_stag=-1, roll_coords=False))\n",
    "        z_destaggered = 0.5 * (ds['GEOPOT'] + ds['GEOPOT'].roll(bottom_top_stag=-1, roll_coords=False))\n",
    "        \n",
    "        # Trim the last row/column if needed to match other variables' dimensions\n",
    "        u_destaggered = u_destaggered.isel(west_east_stag=slice(None, -1))\n",
    "        v_destaggered = v_destaggered.isel(south_north_stag=slice(None, -1))\n",
    "        w_destaggered = w_destaggered.isel(bottom_top_stag=slice(None, -1))\n",
    "        z_destaggered = z_destaggered.isel(bottom_top_stag=slice(None, -1))\n",
    "\n",
    "        u_destaggered = u_destaggered.rename({'west_east_stag' : 'lon'})\n",
    "        v_destaggered = v_destaggered.rename({'south_north_stag' : 'lat'})\n",
    "        w_destaggered = w_destaggered.rename({'bottom_top_stag' : 'level'})\n",
    "        z_destaggered = z_destaggered.rename({'bottom_top_stag' : 'level'})\n",
    "\n",
    "        ds['U'] = u_destaggered\n",
    "        ds['V'] = v_destaggered\n",
    "        ds['W'] = w_destaggered\n",
    "        ds['GEOPOT'] = w_destaggered\n",
    "\n",
    "        # Add 300. to make it properly Kelvins. \n",
    "        ds['T']+=300. \n",
    "        \n",
    "        # Latitude and longitude are expected to be 1d vectors. \n",
    "        #lat_1d = ds['latitude'].isel(lon=0, time=0)\n",
    "        #lon_1d = ds['longitude'].isel(lat=0, time=0)\n",
    "        #ds = ds.drop_vars(['lon', 'lat'])\n",
    "        \n",
    "        # Assign the 2D versions of 'xlat' and 'xlon' back to the dataset as coordinates\n",
    "        ds = ds.assign_coords(lat=lat_1d, lon=lon_1d)\n",
    "\n",
    "        #ds = ds.drop_vars(['longitude', 'latitude'])\n",
    "\n",
    "        # Convert negative longitude values to 0-360 range and update the Dataset\n",
    "        ds['lon'] = xr.where(ds['lon'] < 0, ds['lon'] + 180, ds['lon'])\n",
    "\n",
    "        # Formating the time dimension for the graphcast code. \n",
    "        # Define the start time for your dataset\n",
    "        start_time = pd.Timestamp(f'{date}{init_time}')\n",
    "\n",
    "        num_time_points = ds.sizes['time']\n",
    "\n",
    "        # Generate the datetime range\n",
    "        time_range = pd.date_range(start=start_time, periods=num_time_points, freq=time_resolution)\n",
    "\n",
    "        ds['time'] = time_range\n",
    "\n",
    "        # Only adding a fake datetime so that the GraphCast code can drop it :) \n",
    "        ds = ds.assign_coords(datetime=time_range)\n",
    "\n",
    "        # Convert 'time' dimension to timedeltas from the first time point\n",
    "        time_deltas = (ds['time'] - ds['time'][0]).astype('timedelta64[ns]')\n",
    "        ds['time'] = time_deltas\n",
    "\n",
    "        # Add level coordinate to the dataset \n",
    "        ds = ds.assign_coords(level=ds.level)\n",
    "\n",
    "        # Assuming 'lat' and 'lon' are the coordinate names for the grid dimensions\n",
    "        n_lat, n_lon = ds.dims['lat'], ds.dims['lon']\n",
    "    \n",
    "        size = 150\n",
    "        start_lat, start_lon = (n_lat - size) // 2, (n_lon - size) // 2\n",
    "        end_lat, end_lon = start_lat + size, start_lon + size\n",
    "        \n",
    "        # Subsetting the dataset to the central size x size grid\n",
    "        ds_subset = ds.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon))\n",
    "        \n",
    "        ## Define encoding with compression\n",
    "        encoding = {var: {'zlib': True, 'complevel': 5} for var in ds.data_vars}\n",
    "        ds_subset.to_netcdf(fname, encoding=encoding)\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return None \n",
    "\n",
    "    \n",
    "date = '20200521'\n",
    "init_time = '0000'\n",
    "mem = 2\n",
    "process_dataset(date, init_time, mem, drop_vars)\n",
    "\n",
    "from wofscast import data_utils\n",
    "from wofscast import my_graphcast as graphcast\n",
    "\n",
    "# Create the training and target datasets. \n",
    "input_variables = ['U', 'V', 'W', 'T']#, 'P', 'REFL_10CM', 'UP_HELI_MAX']\n",
    "target_variables = ['U', 'V', 'W', 'T']#, 'P', 'REFL_10CM', 'UP_HELI_MAX']\n",
    "forcing_variables = [\"XLAND\"]\n",
    "# Not pressure levels, but just vertical array indices at the moment. \n",
    "pressure_levels = np.arange(0, 40) #list(np.arange(0,40,2))\n",
    "# Loads data from the past 20 minutes (2 steps) and \n",
    "# creates a target over the next 10-60 min. \n",
    "input_duration = '20min'\n",
    "train_lead_times = slice('10min', '60min') \n",
    "\n",
    "\n",
    "task_config = graphcast.TaskConfig(\n",
    "      input_variables=input_variables,\n",
    "      target_variables=target_variables,\n",
    "      forcing_variables=forcing_variables,\n",
    "      pressure_levels=pressure_levels,\n",
    "      input_duration=input_duration,\n",
    "  )\n",
    "\n",
    "# Example usage:\n",
    "chunk_size = 3  # The size of each chunk\n",
    "overlap = 2     # The overlap between consecutive chunks\n",
    "\n",
    "data_paths = glob(os.path.join('/work/mflora/wofs-cast-data/eval_datasets/dataset*.nc'))\n",
    "data_paths.sort()\n",
    "\n",
    "train_input_list = []\n",
    "train_target_list = []\n",
    "train_forcing_list = []\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for path in data_paths:\n",
    "    dataset = xr.load_dataset(path)\n",
    "    \n",
    "    # @title Extract training and eval data\n",
    "    example_batch = dataset.expand_dims(dim='batch', axis=0)\n",
    "\n",
    "    _train_inputs, _train_targets, _train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "            example_batch, target_lead_times=train_lead_times,\n",
    "            **dataclasses.asdict(task_config))\n",
    "\n",
    "    train_input_list.append(_train_inputs)\n",
    "    train_target_list.append(_train_targets)\n",
    "    train_forcing_list.append(_train_forcings)\n",
    "    \n",
    "train_inputs = xr.concat(train_input_list, dim='batch')\n",
    "train_targets = xr.concat(train_target_list, dim='batch')\n",
    "train_forcings = xr.concat(train_forcing_list, dim='batch')\n",
    "\n",
    "base_path = '/work/mflora/wofs-cast-data/train_datasets'\n",
    "# Save to NetCDF files\n",
    "print('Saving the training datasets...')\n",
    "train_inputs.to_netcdf(os.path.join(base_path, 'eval_inputs.nc'))\n",
    "train_targets.to_netcdf(os.path.join(base_path, 'eval_targets.nc'))\n",
    "train_forcings.to_netcdf(os.path.join(base_path, 'eval_forcings.nc'))\n",
    "\n",
    "\n",
    "'''    \n",
    "    \n",
    "#process_dataset(dates[0], init_times[0], 1, drop_vars)    \n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_dataset, date, init_time, \n",
    "                               mem, drop_vars) for date in dates for mem in mems for init_time in init_times]\n",
    "    for future in tqdm(futures, desc=\"Processing datasets\", total=total_files):\n",
    "        future.result()  # Wait for all futures to complete\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "from wofscast import data_utils\n",
    "from wofscast import my_graphcast as graphcast\n",
    "\n",
    "# Create the training and target datasets. \n",
    "input_variables = ['U', 'V', 'W', 'T']#, 'P', 'REFL_10CM', 'UP_HELI_MAX']\n",
    "target_variables = ['U', 'V', 'W', 'T']#, 'P', 'REFL_10CM', 'UP_HELI_MAX']\n",
    "forcing_variables = [\"XLAND\"]\n",
    "# Not pressure levels, but just vertical array indices at the moment. \n",
    "pressure_levels = np.arange(0, 40) #list(np.arange(0,40,2))\n",
    "# Loads data from the past 20 minutes (2 steps) and \n",
    "# creates a target over the next 10-60 min. \n",
    "input_duration = '20min'\n",
    "train_lead_times = slice('10min', '60min') \n",
    "\n",
    "\n",
    "task_config = graphcast.TaskConfig(\n",
    "      input_variables=input_variables,\n",
    "      target_variables=target_variables,\n",
    "      forcing_variables=forcing_variables,\n",
    "      pressure_levels=pressure_levels,\n",
    "      input_duration=input_duration,\n",
    "  )\n",
    "\n",
    "# Example usage:\n",
    "chunk_size = 3  # The size of each chunk\n",
    "overlap = 2     # The overlap between consecutive chunks\n",
    "\n",
    "data_paths = glob(os.path.join('/work/mflora/wofs-cast-data/datasets/dataset*.nc'))\n",
    "data_paths.sort()\n",
    "\n",
    "train_input_list = []\n",
    "train_target_list = []\n",
    "train_forcing_list = []\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for path in data_paths:\n",
    "    dataset = xr.load_dataset(path)\n",
    "    \n",
    "    # @title Extract training and eval data\n",
    "    example_batch = dataset.expand_dims(dim='batch', axis=0)\n",
    "\n",
    "    _train_inputs, _train_targets, _train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "            example_batch, target_lead_times=train_lead_times,\n",
    "            **dataclasses.asdict(task_config))\n",
    "\n",
    "    train_input_list.append(_train_inputs)\n",
    "    train_target_list.append(_train_targets)\n",
    "    train_forcing_list.append(_train_forcings)\n",
    "    \n",
    "train_inputs = xr.concat(train_input_list, dim='batch')\n",
    "train_targets = xr.concat(train_target_list, dim='batch')\n",
    "train_forcings = xr.concat(train_forcing_list, dim='batch')\n",
    "    \n",
    "print(\"All Examples:  \", example_batch.dims.mapping)\n",
    "print(\"*\"*80)\n",
    "print(\"Train Inputs:  \", train_inputs.dims.mapping)\n",
    "print(\"Train Targets: \", train_targets.dims.mapping)\n",
    "print(\"Train Forcings:\", train_forcings.dims.mapping)\n",
    "\n",
    "base_path = '/work/mflora/wofs-cast-data/train_datasets'\n",
    "# Save to NetCDF files\n",
    "print('Saving the training datasets...')\n",
    "train_inputs.to_netcdf(os.path.join(base_path, 'train_inputs.nc'))\n",
    "train_targets.to_netcdf(os.path.join(base_path, 'train_targets.nc'))\n",
    "train_forcings.to_netcdf(os.path.join(base_path, 'train_forcings.nc'))\n",
    "\n",
    "print('Computing and saving the normalization datasets...')\n",
    "full_dataset = xr.concat([train_inputs, train_forcings], dim='batch')\n",
    "\n",
    "# Compute the global mean and standard deviation by level\n",
    "mean_by_level = full_dataset.mean(dim=['time', 'lat', 'lon', 'batch'])\n",
    "stddev_by_level = full_dataset.std(dim=['time', 'lat', 'lon', 'batch'])\n",
    "\n",
    "# For differences, first compute differences by time within each dataset, then concatenate\n",
    "time_diffs = full_dataset.diff(dim='time')\n",
    "\n",
    "# Compute the global standard deviation of the differences by level\n",
    "diffs_stddev_by_level = time_diffs.std(dim=[ 'time', 'lat', 'lon', 'batch'])\n",
    "\n",
    "# Save to NetCDF files\n",
    "base_path = '/work/mflora/wofs-cast-data/normalization_stats/'\n",
    "\n",
    "mean_by_level.to_netcdf(os.path.join(base_path, 'mean_by_level.nc'))\n",
    "stddev_by_level.to_netcdf(os.path.join(base_path, 'stddev_by_level.nc'))\n",
    "diffs_stddev_by_level.to_netcdf(os.path.join(base_path, 'diffs_stddev_by_level.nc'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45c12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
